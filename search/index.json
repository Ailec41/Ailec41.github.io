[{"content":"开始学cs231n了，期望是一周3-4节，尽量4周完成掉\n课程主页:https://cs231n.stanford.edu/\n作业:https://cs231n.stanford.edu/schedule.html\n图像分类 图像通常由数据矩阵定义，更一般地说是数据张量，识别图像对于机器来说是个很大的挑战，举个例子，人类不管从什么角度看一个物体他都是一样的，但是当一个摄像机对准一个物体并转动，像素值实时都在改变，除此之外，光照，物体遮挡等等对于图像的识别来说都是挑战\n机器学习采用了数据驱动的方法：\n收集图像及其标签的数据集 使用机器学习算法训练分类器 在新图像上评估分类器 下面是分别对应步骤2和3的接口\nNearest Neighbor Classifier 设定一个距离函数，对于一对图像（query data和training data），返回一个定义两者相似度的值\n下面是两种常见的计算距离的方式\n首先是L1距离，定义为两个图像所有像素差绝对值的总和\n不过我们不难发现，训练函数是$O(1)$的，而预测函数是$O(n)$的，这并不是我们想要的\n因此我们把Nearest Neighbor自然推广到k-Nearest Neighbor\n","date":"2025-11-25T00:00:00Z","image":"https://lunatide.tech/p/cs231-%E7%AC%AC%E4%BA%8C%E8%AE%B2-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/pic1_hu_bf2b74b0811abddd.jpg","permalink":"https://lunatide.tech/p/cs231-%E7%AC%AC%E4%BA%8C%E8%AE%B2-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/","title":"CS231 第二讲 图像分类"},{"content":"参数的更新 ","date":"2025-11-25T00:00:00Z","image":"https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E4%B8%8E%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%9A%84%E6%8A%80%E5%B7%A7/pic3_hu_2daba19cf5c34236.jpg","permalink":"https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E4%B8%8E%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%9A%84%E6%8A%80%E5%B7%A7/","title":"鱼书笔记-与学习相关的技巧"},{"content":"计算图 用计算图求解 我们先来看一个简单的问题\n问题：太郎在超市买了2个100日元一个的苹果，消费税是10%，请计算支付金额\n如何用计算图表示，这个非常简单，小学生都能看懂\n或者也可以把运算的数字放在圆圈外面，如下图\n上面说的这种便是正向传播运算，也就是我们的正常运算的逻辑，但是这章的主题是反向传播，我们来看看这是什么\n反向传播 加法节点的反向传播 以z=x+y为例，左图为正向传播，右图为反向传播\n乘法节点的反向传播 以z=xy为例\n回到开头的例子 所以重新思考开头的那个买苹果的例子，要解的就是苹果的价格，苹果的个数，消费税这三个变量之间各自如何影响最终支付的金额，相当于求“支付金额关于苹果价格的导数”，“支付金额关于苹果个数的导数“，”支付金额关于消费税的导数”，反向传播的过程如下图\n如图， 苹果价格的导数是2.2，苹果个数的导数是110，消费税的导数是200，意思就是，如果消费税和苹果的价值增长同样的值，消费税将对最终金额产生200倍左右的影响，苹果的价格将产生2.2倍大小的影响（不过这个例子在中两者的量纲不同）\n简单层的实现 本节用python实现购买苹果的例子\n乘法层的实现 层的实现中有两个共通的方法forwar()和backward()。forward()对应正向传播，backward()对应反向传播。\n然后来实现乘法层\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class MulLayer: def __init__(self): self.x = None self.y = None def forward(self,x,y): self.x = x self.y = y out = x * y return out def backward(self,dout): dx = dout * self.y dy = dout * self.x return dx,dy __init__()中会初始化实例变量x和y，它们用于保存正向传播时的输出值。forward()接收x和y两个参数，将它们相乘后输出。backward()将从上游传来的导数(dout)乘以正向传播的翻转值，然后传给下游\n加法层的实现 1 2 3 4 5 6 7 8 9 10 11 12 class AddLayer: def __init__(self): pass def forward(self,x,y): out = x + y return out def backward(self,dout): dx = dout * 1 dy = dout * 1 return dx,dy 加法层不需要初始化，实现非常简单\n例子 接下来看个实际操作的例子\n上图可以像如下一样实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apple = 100 apple_num = 2 orange = 150 orange_num = 3 tax = 1.1 #layer mul_apple_layer = MulLayer() mul_apple_layer = MulLayer() add_apple_orange_layer = AddLayer() mul_tax_layer = MulLayer() # forward apple_price = mul_apple_layer.forward(apple,app_num) orange_price = mul_orange_layer.forward(orange,orange_num) all_price = add_apple_orange_layer.forward(apple_price,orange_price) price = mul_tax_layer.forward(all,price,tax) #backward dprice = 1 dall_price,dtax = mul_tax_layer.backward(dprice) dapple_price,dorange_nprice =add_apple_orange_layer.backward(dall_price) dorange,dorange_num = mul_orange_layer.backward(dorange_price) dapple,dapple_num = mul_apple_layer.backward(dapple_price) print(price) print(dapple_num,dapple,dorange_num,dtax) 激活函数层的实现 ReLU层 激活函数ReLU由下式表示 $$ y = \\begin{cases} x \u0026 (x\u003e0) \\\\ 0 \u0026 (x \\le 0) \\end{cases}\\tag{1} $$ 通过式(1)，可以求出y关于x的导数，如下式 $$ \\frac{\\partial y}{\\partial x} = \\begin{cases} 1 \u0026 (x\u003e0) \\\\ 0 \u0026 (x \\le 0) \\end{cases}\\tag{2} $$ 接下来实现一下ReLU层\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class ReLU: def __init__(self): self.mask = None def forward(self,x): self.mask = (x \u0026lt;= 0) out = x.copy() out[self.mask] = 0 def backward(self,dout): dout[self.mask] = 0 dx = dout return dx ReLU由实例变量mask。这个变量mask是由True/False构成的NumPy数组，它会把正向传播时输入的x的元素中小于等于0的地方保存为True，其他地方（大于0的元素）保存为False\nSigmoid层 接下来来实现一下sigmoid函数，sigmoid函数如下式所示 $$ y = \\frac{1}{1 + \\exp(-x)}\\tag{3} $$ 用计算图表示上式，如下所示\n然后我们来看下反向传播是怎么样的\n上图就是Sigmoid函数的反向传播过程，如果你看懂了上面的内容相信这个不难理解\n我们在反向传输的过程中只需要专注于它的输入和输出就可以，不用在意繁琐的过程\n输出的结果此外， $\\frac{\\partial L}{\\partial y} y^{2} \\exp(-x)$ 可以进一步整理如下：\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial y} y^{2} \\exp(-x) \u0026= \\frac{\\partial L}{\\partial y} \\frac{1}{(1+\\exp(-x))^{2}} \\exp(-x) \\\\ \u0026= \\frac{\\partial L}{\\partial y} \\frac{1}{1+\\exp(-x)} \\frac{\\exp(-x)}{1+\\exp(-x)} \\\\ \u0026= \\frac{\\partial L}{\\partial y} \\, y (1-y) \\end{aligned}\\tag{4} $$ 实现一下Sigmoid层\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Sigmoid: def __init__(self): self.out = Nonoe def forward(self,x): out = 1 / (1 + np.exp(-x)) self.out = out return out def backward(self,dout): dx = dout * (1.0 - self.out) * self.out return dx Affine/Softmax层的实现 Affine层 神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的积乘运算(NumPy中是np.dot)\n神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”。因此，这里将进行仿射变换的处理实现称为“Affine层”\n将这里进行的求矩阵的乘积和偏置的和的运算用计算图表示出来。将乘积运算用“dot”节点表示的话，则np.dot(X,W) + B的运算可以用下图的计算图来表示出来，另外，在各个变量的上方标记了它们的形状\n上图是比较简单的计算图，不过要注意X,W,B是矩阵\n考虑上图的反向传播，以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。\n我们可以写出计算图的反向传播，如下图\n观察一下上图中各个变量的形状，X和$\\frac{\\partial L}{\\partial \\mathbf{X}}$形状相同，W和$\\frac{\\partial L}{\\partial \\mathbf{W}}$，形状相同，从下式就可以看出X和$\\frac{\\partial L}{\\partial \\mathbf{X}}$形状相同 $$ \\mathbf{X} = (x_0, x_1, \\cdots, x_n)\\\\ \\frac{\\partial L}{\\partial \\mathbf{X}} = \\left( \\frac{\\partial L}{\\partial x_0}, \\frac{\\partial L}{\\partial x_1}, \\cdots, \\frac{\\partial L}{\\partial x_n} \\right)\\tag{5} $$批版本的Affine层 前面介绍的Affine层的输入X是以单个数据为对象的。现在我们考虑N个数据一起进行正向传播的情况\n下图是批版本的affine层的计算图\n现在输入X的形状是(N,2)。之后就和前面一样\n正向传播时，偏置被加到$X·W$的各个数据上。比如，N=2时，偏置会分别加到这两个数据上，因此反向传播时，各个数据的反向传播的值需要汇总为偏置的元素\nAffine的实现如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Affine: def __init__(self,W,b): self.W = W self.b = b self.x = None self.dW = None delf.db = None def forward(self,x): self.x = x out = np.dot(x,self.W) + self.b return out def backward(self,dout): dx = np.dot(dout,self.W.T) self.dW = np.dot(self,x.T,dout) self.db = np.sum(dout,axis=0) return dx Softmax-with-Loss层 之前说过softmax函数会将输入值正规化（将输出值的和调整为1）然后再输出。另外，因为手写数字识别要进行10类分类，所以向Softmax层的输入也有10个\n下面来实现Softmax层，计算图如下图所示\n上图的计算图可以简化成下图\n上图的计算图中，softmax函数记为Softmax层，交叉熵误差记为Cross Entropy error层。这里假设要进行三类分类，从前面的层接收三个输入，Softmax层将输入(a1,a2,a3)正规化，输出(y1,y2,y3)Cross Entropy Error层接收Softmax的输出(y1,y2,y3)和教师标签(t1,t2,t3)，从这些数据中输出损失L\n上图要注意的是反向传播的结果，Softmax层的反向传播得到了(y1-t1,y2-t2,y3-t3)这样漂亮的结果。由于(y1,y2,y3)是Softmax层的输出，(t1,t2,t3)是监督数据，所以(y1-t1,y2-t2,y3-t3)是Softmax层的输出和教师标签的差分。神经网络会把这个差分表示的误差传递给前面的层。\n神经网络学习的目的就是通过调整权重参数，使神经网络的输出接近教师标签。因此，必须将神经网络的输出与教师标签的误差高效地传递给前面的层\n现在实现一下Softmax-with-Loss层\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class SoftmaxWithLoss: def __init__(self): self.loss = None # 损失 self.y = None # softmax的输出 self.t = None # 监督数据（one-hot vector） def forward(self, x, t): self.t = t self.y = softmax(x) self.loss = cross_entropy_error(self.y, self.t) return self.loss def backward(self, dout=1): batch_size = self.t.shape[0] dx = (self.y - self.t) / batch_size r\treturn dx ","date":"2025-11-22T00:00:00Z","image":"https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95/pic3_hu_2daba19cf5c34236.jpg","permalink":"https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95/","title":"鱼书笔记-误差反向传播法"},{"content":"从数据中学习 神经网络的特征就是从数据中学习（由数据自动决定权重参数的值）\n数据驱动 我们接着上一章最后手写数字识别的话题，思考一下会发现如果设计一个能自动识别5的算法还是挺困难的（至少我是这样认为的），所以我们应该考虑通过有效利用数据来解决这个问题，一种方案是从图像中提取特征量，再用机器学习技术学习这些特征量的模式\n机器学习的方法中，由机器从收集到的数据中找到规律性。但是将图像转换为向量时使用的特征量仍是由人设计的，对于不同的问题，必须使用合适的特征量，才能得到好的结果\n还有一种是神经网络（深度学习）的方法，该方法不存在人为介入，神经网络会直接学习图像本身\n训练数据和测试数据 机器学习中把数据分成训练数据和测试数据两部分，首先用训练数据进行学习，寻找最优的参数，然后用测试数据评价训练得到的模型的实际能力，为了正确评价模型的泛化能力，就必须划分训练数据和测试数据，训练数据也被称作监督数据\n泛化能力是指处理未被观察过的数据的能力。机器学习的目标就是为了提高泛化能力\n因此，仅仅用一个数据集去学习和评价参数，无法正确评价，只用某个数据集过度拟合的状态称为过拟合\n损失函数 神经网络的学习通过某个指标来表示现在的状态。然后以这个指标为基准，寻找最优权重参数。这个指标被称为损失函数。损失函数可以使用任意参数，但一般用均方误差和交叉熵误差等。\n均方误差 如下式 $$ E = \\frac{1}{2} \\sum_k (y_k - t_k) ^ 2 \\tag{1} $$ 这里$y_k$是表示神经网络的输出，$t_k$是表示监督数据，$k$表示数据的维数，如式(1)所示，均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方，再求总和。python实现均方误差的实现方式如下所示\n1 2 def mean_squared_error(y, t): return 0.5 * np.sum((y - t)**2) 交叉熵误差 交叉熵误差如下式所示 $$ E = - \\sum_k (t_k \\log{y_k}) \\tag{2} $$ $y_k$是神经网络的输出，$t_k$是正确解标签(采用one-hot表示)。交叉熵误差的值是由正确解标签所对应的输出结果决定的。\n根据对数函数的性质我们可以知道，正确解标签对应的输出越大，式(2)的值就越靠近0；输出为1时，交叉熵的误差为0。如果正确解标签对应的输出较小，(2)的值就越大。\n下面实现一下交叉熵误差\n1 2 3 def cross_entropy_error(y,t): delta = 1e - 7 return -np.sum(t * np.log(y + delta)) y和t在这里是NumPy数组，加上一个delta是为了防止-inf的发生\nmini-batch学习 前面说的都是单个数据的损失函数。如果要求所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成下面的式(3) $$ E = -\\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk}\\,\\log y_{nk} \\tag{3} $$ 假设一共有N个数据，$t_{nk}$表示第n个数据的第k个元素的值\n这个式子就是把单个数据的损失函数的式扩大到了N份数据，不过最后还要除以N进行正规化。\nMNIST数据集的训练数据有60000个，用全部数据来计算损失函数的值所花费的时间太长，所以我们从中选取一部分。神经网络的学习也是从训练数据中选出一批数据（称为mini-batch)，然后对每个mini-batch进行学习。\nmini-batch版交叉熵误差的实现 对于mini-batch的交叉熵误差，只要改良一下之前实现对应单个数据的交叉熵误差就可以。这里实现一个可以同时处理单个数据和批量数据两种情况的函数\n1 2 3 4 5 6 7 def cross_entropy_error(y, t): if y.ndim == 1: t = t.reshape(1,t.size) y = y.reshape(1,y.size) batch_size = y.shape[0] return -np.sum(np,log(y[np.arange(batch_size),t] + 1e - 7)) / batch_size 这里，y是神经网络的输出，t是监督数据。y的维度为1时，即求单个数据的交叉熵误差时，需要改变数据的形状。并且，当输入为mini-batch时，要用batch的个数进行正规化，计算单个数据的平均交叉熵误差\n此外，当监督数据时标签形式(非one-hot表示，而是像\u0026quot;2\u0026quot; \u0026ldquo;7\u0026quot;这样的)交叉熵误差可以如下实现\n1 2 3 4 5 6 7 def cross_entropy_error(y, t): if y.ndim == 1: t = t.reshape(1,t.size) y = y.reshape(1,y.size) batch_size = y.shape[0] return -np.sum(np.sum(np.log(y[np.arrange(batch_size),t] + 1e - 7)) / batch_size 由于one-hot表示中t为0的元素的交叉熵误差也为0，因此针对这些元素的计算可以忽略。只要可以获得神经网络在正确解标签的输出，就可以计算交叉熵误差,t为one-hot表示时通过t * np.log(y)计算的地方t为标签形式时，可以用np.log(y[np.arange(batch_size),t])表示实现相同的处理\n为什么要设定损失函数 假设有一个神经网络，对其中一个权重参数的损失函数求导，如果这个导数的值为负，说明使该权重参数向正正方向改变，可以减小损失函数的值；反之亦然，以及当导数的值为0时候，无论权重参数往哪个方向，损失函数的值都不会改变。而如果用识别精度作为指标，则参数的导数在绝大多数地方都为0\n梯度法 梯度的方向不一定指向最小值，但是沿着梯度的方向能够最大限度地减小函数的值\n梯度法是什么，就是让函数的取值沿着梯度的方向前进一段距离，在新的地方重新求梯度，然后再沿着梯度方向前进，像这样反复，逐渐减小函数值，然后我们用数学式来表示梯度法，如下式(4) $$ x_0 = x_0 - \\eta \\frac{\\partial f}{\\partial x_0}\\\\ x_1 = x_1 - \\eta \\frac{\\partial f}{\\partial x_1}\\tag{4} $$上式的$\\eta$表示更新量，在神经网络的学习中，称为学习率，决定了在一次学习中，应该学习多少，以及在多大程度上更新参数\n接下来用python实现下梯度下降法\n1 2 3 4 5 6 7 def gradient_descet(f,init_x,lr = 0.01,step_num = 100): x = init_x; for i in range(step_num): grad = numerical_gradient(f,x) x -= lr * grad return x 参数f是要进行最优化的函数，init_x是初始值，lr是学习率，step_num是梯度法的重复次数，numerical_gradient(f,x)会求函数的梯度\n神经网络的梯度 神经网络的学习也要求梯度，这里所说的梯度是指损失函数关于权重参数的梯度，例如一个形状2x3的权重$W$的神经网络，损失函数用L表示。此时，梯度可以用$\\frac{\\partial L}{\\partial \\mathbf{W}}$表示 $$ \\mathbf{W} = \\begin{pmatrix} w_{11} \u0026 w_{12} \u0026 w_{13} \\\\ w_{21} \u0026 w_{22} \u0026 w_{23} \\end{pmatrix}\\\\ \\frac{\\partial L}{\\partial \\mathbf{W}} = \\begin{pmatrix} \\frac{\\partial L}{\\partial w_{11}} \u0026 \\frac{\\partial L}{\\partial w_{12}} \u0026 \\frac{\\partial L}{\\partial w_{13}} \\\\ \\frac{\\partial L}{\\partial w_{21}} \u0026 \\frac{\\partial L}{\\partial w_{22}} \u0026 \\frac{\\partial L}{\\partial w_{23}} \\end{pmatrix}\\tag{5} $$","date":"2025-11-17T00:00:00Z","image":"https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/pic3_hu_2daba19cf5c34236.jpg","permalink":"https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/","title":"鱼书笔记-神经网络的学习"},{"content":"以下内容皆基于鱼书《深度学习入门基于python的理论与实现》\n3层神经网络的实现 开始进行神经网络的实现，以下图的三层神经网络为例\n符号确认 首先导入符号$w_{12}^{(1)}$, $a_{1}^{(1)}$等，如下图，权重和隐藏层的神经元右上角有一个\u0026quot;(1)\u0026quot;，它表示权重和神经元的层号，此外，权重右下角的两个数字，它们是后一层的神经元和前一层的神经元的索引号，比如$w_{12}^{(1)}$表示前一层的第二个神经元$x_2$到后一层的第1个神经元$a_{1}^{(1)}$的权重。权重右下角按照\u0026quot;后一层的索引号、前一层的索引号\u0026quot;的顺序排序\n各层间信号传递的实现 上图增加了表示偏置的神经元\u0026quot;1\u0026quot;。偏置的右下角的索引号只有一个因为前一层的偏置神经元只有一个\n现在通过加权信号和偏置的和计算表示$a_{1}^{(1)}$。 $$ a_{1}^{(1)} = w_{11}^{(1)} x_{1} + w_{12}^{(1)} x_{2} + b_{1}^{(1)}\\tag{8} $$ 如果用矩阵的乘法运算，则可以将第1层的加权和表示成下面的式(9) $$ A^{(1)} = XW^{(1)} + B^{(1)} \\tag{9} $$ 其中，$A^{(1)}$、$X$、$B^{(1)}$、$W^{(1)}$ 如下所示： $$ A^{(1)} = \\begin{pmatrix} a_{1}^{(1)} \u0026 a_{2}^{(1)} \u0026 a_{3}^{(1)} \\end{pmatrix}, \\quad X = \\begin{pmatrix} x_1 \u0026 x_2 \\end{pmatrix}, \\quad B^{(1)} = \\begin{pmatrix} b_{1}^{(1)} \u0026 b_{2}^{(1)} \u0026 b_{3}^{(1)} \\end{pmatrix} $$$$ W^{(1)} = \\begin{pmatrix} w_{11}^{(1)} \u0026 w_{21}^{(1)} \u0026 w_{31}^{(1)} \\\\ w_{12}^{(1)} \u0026 w_{22}^{(1)} \u0026 w_{32}^{(1)} \\end{pmatrix} $$然后用NumPy多维数组来实现式(9)，输入信号，权重，偏置设置成任意值\n1 2 3 4 5 6 7 8 9 X = np.array([1.0,0.5]) W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]]) B1 = np.array([0,1,0,2,0,3]) print(W1.shape) # (2,3) print(X.shape) #(2,) print(B1.shape) #(3.) A1 = np.dot(X,W1) + B1 W1是2x3的数组，X是元素个数为2的一维数组。这里，W1和X的对应维度的元素个数也保持了一致。\n然后我们用python来实现第一层激活函数的计算过程\n1 2 3 4 Z1 = sigmoid(A1) print(A1) print(Z1) 这里说的sigmoid函数就是之前定义的那个，它会接收NumPy数组，然后返回元素个数相同的NumPy数组\n下面我们来实现第1层到第2层的信号传递\n1 2 3 4 5 6 7 8 9 W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]]) B2 = np.array([0.1,0.2]) print(Z1.shape) #(3,) print(W2.shape) #(3,2) print(B2.shape) #(2,) A2 = np.dot(Z1,W2) + B2 Z2 = sigmoid(A2) 除了第一层的输出变成了第二层的输入，这个实现和刚才的一样\n最后是第二层到输出层的信号传递，输出层的实现也和之前的实现基本相同，不过，最后的激活函数和之前的隐藏层有所不同\n1 2 3 4 5 6 7 8 def identity_function(x): return x W3 = np.array([0.1,0.3],[0.2,0.4]) B3 = np.array([0.1,0.2]) A3 = np.dot(Z2,W3) + B3 Y = identity_function(A3) 这里定义了identity_function()函数（恒等函数），并将其作为输出层的激活函数。\n代码总结 按照神经网络的实现惯例，把权重记为大写字母W1，其他都用小写字母表示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def init_network(): network = {} network[\u0026#39;W1\u0026#39;] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) network[\u0026#39;b1\u0026#39;] = np.array([0.1, 0.2, 0.3]) network[\u0026#39;W2\u0026#39;] = np.array([[0.1, 0.4],[0.2, 0.5],[0.3, 0.6]]) network[\u0026#39;b2\u0026#39;] = np.array([0.1, 0.2]) network[\u0026#39;W3\u0026#39;] = np.array([[0.1, 0.3],[0.2, 0.4]]) network[\u0026#39;b3\u0026#39;] = np.array([0.1, 0.2]) return network def forward(network,x): W1, W2, W3 = network[\u0026#39;W1\u0026#39;],network[\u0026#39;W2\u0026#39;],network[\u0026#39;W3\u0026#39;] b1, b2, b3 = network[\u0026#39;b1\u0026#39;],network[\u0026#39;b2\u0026#39;],network[\u0026#39;b3\u0026#39;] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = identity_function(a3) return y; network = init_network() x = np.array([1.0, 0.5]) y = forward(network, x) print(y) # [ 0.31682708 0.69627909] 这里定义了init_network()和forward()函数，init_network()函数会进行权重和偏置的初始化，并将它们保存在字典变量network中。forward()函数中则封装了将输入信号转换为输出信号的处理过程\n输出层的设计 神经网络要根据情况改变输出层的激活函数。一般而言，回归问题用恒等函数，分类问题用softmax函数。\n恒等函数和softmax函数 恒等函数会将输入按原样输出\n分类问题中的softmax函数可以用下面的式(10)表示 $$ y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^{n} \\exp(a_i)}\\tag{10} $$ 上式表示假设输出层共有n个神经元，计算第k个神经元的输出$y_k$，如式(10)所示，softmax函数的分子是输入信号$a_k$的指数函数，分母是所有输入信号的指数函数的和\n接下来来实现softmax函数。\n1 2 3 4 5 6 def softmax(a): exp_a = np.exp(a) sum_exp_a = np.sum(exp_a) y = exp_a / sub_exp_a return y; 实现softmax函数时的注意事项 上面的softmax函数在计算上有一定的缺陷，就是溢出的问题，softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大，比如$e^{1000}$的结果会返回一个表示无穷大大inf，在这些超大值之间进行除法运算，结果会出现不确定的情况，softmax可以像如下(11)改进 $$ \\begin{aligned} y_k \u0026= \\frac{\\exp(a_k)}{\\sum_{i=1}^{n} \\exp(a_i)} = \\frac{C \\exp(a_k)}{C \\sum_{i=1}^{n} \\exp(a_i)} \\\\[6pt] \u0026= \\frac{\\exp(a_k + \\log C)}{\\sum_{i=1}^{n} \\exp(a_i + \\log C)} \\\\[6pt] \u0026= \\frac{\\exp(a_k + C')}{\\sum_{i=1}^{n} \\exp(a_i + C')} \\end{aligned} \\tag{11} $$ 先在分子和分母上都乘以C（一个任意的常数），然后把C移动到指数函数中，记为$log C$。最后把$logC$替换为另外一个符号$C'$\n综上，我们来实现下最终版的softmax函数\n1 2 3 4 5 6 7 def softmax(a): c = np.max(a) exp_a = np.exp(a - c) sum_exp_a = np_sum(exp_a) y = exp_a / sum_exp_a return y softmax函数的特征 输出总和为1，因为这个性质我们才可以把softmax函数的输出解释为“概率” 使用了softmax函数各个元素之间的大小关系也不会改变，因为exp是单调递增的 神经网络一般只会把输出值最大的神经元所对应的类别作为识别结果。使用softmax函数输出值最大的神经元的位置也不会变，因此输出层的softmax函数一般会被忽略 输出层的神经元数量 输出层的神经元数量需要根据待解决的问题来决定。对于分类问题，输出层的神经元数量一般设定为类别的数量。比如，对于某个输入图像，预测是图中的数字0到9中的哪个的问题，可以把输出层的神经元设定为10个，然后把这十个神经元按照从上到下，从0-9依次编号，并且值用不同的灰度表示，颜色越深，输出的值就越大\n手写数字识别 开始解决实际问题，假设学习已经全部结束，我们使用学习到的参数，先实现神经网络的“推理处理”。这个推理处理也称为神经网络的前向传播（感觉这块是看到现在为止唯一一块不太懂的部分）\nMNIST数据集 MNIST数据集是由0到9的数字图像构成的。训练图像有6万张，测试图像有1万张，这些图像可以用于学习与推理。MNIST数据集的一般使用方法是，先用训练图像进行学习，再用学习到的模型度量能在能在多大程度上对测试图像进行正确的分类\nMNIST的图像数据是28像素x28像素的灰度图像（1通道），各个像素的取值在0到255之间。每个图像都相应地标有“7” “2” “1”等标签。\n从数据中学习 数据驱动 如何实现数字“5”的识别，如果要设计一个能将5正确分类的程序\n","date":"2025-11-12T00:00:00Z","image":"https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/pic3_hu_2daba19cf5c34236.jpg","permalink":"https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/","title":"鱼书笔记-神经网络(下)"},{"content":"以下内容皆基于鱼书《深度学习入门基于python的理论与实现》\n从感知机到神经网络 感知机回顾 用图来表示神经网络，类比感知机，我们把左边的一列称为输入层，最右边的称之为输出层，中间的称为中间层(也称为隐藏层，因为神经元肉眼看不见)，我们知道当感知机接受$x_1,x_2$两个输入信号，输出$y$时，可以用如下的数学式来表示 $$ y = \\begin{cases} 0, \u0026 b + w_1 x_1 + w_2 x_2 \\le 0 \\\\ 1, \u0026 b + w_1 x_1 + w_2 x_2 \u003e 0 \\end{cases} \\tag{1} $$ $b$是偏置，用于控制神经元被激活的容易程度，而$w_1,w_2$是表示各个信号的权重的参数，用于控制各个信号的重要性\n我们现在可以通过调用一个函数来替代(1)中分case讨论的情况来简化(1)，改写成如下形式 $$ y = h(b + w_1x_1+ w_2x_2)\\tag{2} $$$$ h(x) = \\begin{cases} 0, \u0026 x \\le 0 \\\\ 1, \u0026 x \u003e 0 \\end{cases}\\tag{3} $$激活函数引入 刚才的h(x)把输入信号的总和转换成了输出信号，h(x)就被称为激活函数(activation function)\n现在进一步改写式(2)，写成如下形式 $$ a = b + w_1x_1 + w_2x_2\\tag{4} $$$$ y = h(a)\\tag{5} $$首先，式(4)计算加权输入信号的和偏置的总和，然后用(5)的h函数转换为输出\n激活函数 sigmoid函数 神经网络中经常使用的一个激活函数就是sigmoid函数 $$ h(x)=\\frac{1}{1+e^{-x}} \\quad (\\text{sigmoid function})\\tag{6} $$ 实际上，感知机和神经网络的主要区别就在于激活函数，其他方面基本都是一样的\n阶跃函数的实现 阶跃函数如(3)所示，当输入超过0时，输出1，否则输出0，可以用如下代码简单实现\n1 2 3 4 5 def step_function(x): if x \u0026gt; 0: return 1 else: return 0 这个代码中参数x只能接受实数。例如不允许step_function(np.array([1.0,2.0]))，所以我们把它修改为支持NumPy数组的实现\n1 2 3 def step_function(x): y = x \u0026gt; 0 return y.astype(np.int) 阶跃函数的图形 接下来我们就用图来表示上面定义的阶跃函数\n1 2 3 4 5 6 7 8 9 10 11 import numpy as np import matplotlib.pylab as plt def step_function(x): return np.array(x \u0026gt; 0,dtype=np.int) x = np.arange(-5.0,5.0,0.1) y = step_function(x) plt.plot(x,y) plt.ylim(-0.1,1.1) #y轴范围 plt.show() sigmoid函数的实现 1 2 def sigmoid(x): return 1 / (1 + np.exp(-x)) 之所以sigmoid函数的实现支持NumPy数组，就是因为NumPy的广播功能，如果在标量和NumPy数组之间进行运算，标量会和NumPy数组的各个元素进行运算，np.exp(-x)会生成NumPy数组，所以1/(1 + np.exp(-x))的运算将会在NumPy数组的各个元素间进行\nsigmoid函数和阶跃函数的比较 观察可以发现，首先区别就是平滑性，sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。因此我们可以知道，感知机的神经元之间流动的是0或1的二元信号，神经网络中流动的是连续的实数值信号。\n然后说一下阶跃函数和sigmoid函数的共同性质，两者的结构均是“输入小时输出接近0；输入大时，输出靠近1”，以及不管输入是什么值，输出信号的值都在0和1中间\n非线性函数 阶跃函数和sigmoid函数都是非线性函数\n神经网络的激活函数必须使用非线性函数，因为如果使用线性函数，加深神经网络的层数就没有意义了（应该很好理解，很多线型函数复合仍然是线性的，就不具体说了）\nReLU函数 最近比较常见的是ReLU函数\nReLU函数在输入大于0时，直接输出该值；在输入小于等于0的时候，输出0\nReLU函数可以表示为以下数学式 $$ h(x) =\\begin{cases} x, \u0026 x \u003e 0 \\\\ 0, \u0026 x \\le 0 \\end{cases} \\tag{7} $$ ReLU函数的实现也非常简单\n1 2 def relu(x): return np.maximum(0,x) 多维数组的运算 多维数组 首先假定有一个一维数组A = np.array[1,2,3,4]，数组的维数可以通过np.ndim得到。数组的形状可以通过实例变量shape获得，A由四个元素构成，是一维的，所以A.shape就是（4，），这个结果是个元组，这个一维数组为了保证和多维一样的格式，所以仍然被写成元组\n矩阵乘法 不再赘述\n神经网络的内积 ","date":"2025-11-10T00:00:00Z","image":"https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic3_hu_2daba19cf5c34236.jpg","permalink":"https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/","title":"鱼书笔记-神经网络(上)"},{"content":"四个Projects都做完了，但是还在刷Leaderboa，有空就update博客内容\n","date":"2025-10-25T00:00:00Z","image":"https://lunatide.tech/p/cmu15445-project01/good_hu_3561d7aa0c4c4079.jpg","permalink":"https://lunatide.tech/p/cmu15445-project01/","title":"CMU15445-Project01"},{"content":"POJ-2318\n","date":"2025-10-16T00:00:00Z","image":"https://lunatide.tech/p/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95%E4%B8%93%E9%A1%B9/codeforce_%E5%89%AF%E6%9C%AC_hu_f47c20acf40349f0.jpg","permalink":"https://lunatide.tech/p/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95%E4%B8%93%E9%A1%B9/","title":"计算几何专项"},{"content":"日常的训练，回顾一下做的几道题，实现不发\nICPC Hangzhou Reginal F Fuzzy Ranking 不太会做，看了一下题解，下面是看完题解后的思路\n直接为任意“前→后”对连边会太多，在同一排列中，只连相邻元素 $a_j\\to a_{j+1}$ 即可：因为一条从前往后的有向路径已经能覆盖该排列中任意“前→后”的可达关系。对所有 $k$ 个排列加边\n对图做一次 Tarjan/Kosaraju缩点，得到每个学校所属的 SCC 编号bel[x]，以及按拓扑序逆序排列的分量列表 scc。把每个榜单按 bel重新映射成“分量编号序列”。\n在某个榜单中，原先相邻的两点之间有边，缩点后相同分量的编号在该榜单上必然连成一段。因此每个榜单都可以被切成若干段，每段恰好由同一个 SCC 的元素组成。这个性质让我们可以把任意区间 $[l,r]$ 拆解为：左端一个不完整段 + 中间若干完整段 + 右端一个不完整段。\n对每个榜单i:\nSegment：对位置轴进行分段。令 Segment= (L, R) 表示pos所在的那段在该榜单上的左右端点下标。线性扫，遇到一段相同编号的区间 $[L,R]$ 就把这对端点填给其中的每个位置。\ndp 前缀：统计从开头到位置 $j$ 为止、同段内能形成的“模糊对”数量。若 $j$ 落在段 $[L,R]$，新增的成对数等于该段里被纳入的元素个数减一，即 $j-L$。因此定义 $$ dp[id][j] = dp[id][j-1] + (j-L). $$ 这样任意区间 $[x,y]$ 的“在同一段内形成的对数”就能通过 $dp$ 差分 $dp[id][y]-dp[id][x]$ 得到。\n总计复杂度$O(nk)$\nCF1841E Fill the Matrix 题目说有一个$n * n$的正方形矩阵，在第$i$列，前$a_i$个格子都是黑色，剩下的格子是白色，在矩阵放入1到m到整数，每个格子最多放一个且黑色格子不能放，定义美丽值为数字 $j$ 和 $j+1$ 位于同一行，且 $j+1$ 位于 $j$ 的右边相邻的格子，求放置m个整数后最大美丽值\n我们一定先选一个最长的白色格子全填进去，直到填满或者数不够。因为如果不继续填等于说最后填的那个数旁边的格子就浪费了，所以关键就在于要把所有行的白色连续横段都计算出来，按长度分组，记为cnt[n]，代表长度n的横段有几个，直接暴力扫迷线维护复杂度是$O(n^2)$的，直接炸了，所以我们用线段树来维护最大值和位置然后再分治来统计cnt数组，相当于构建一颗笛卡尔树，每次把区间按最大值分为左右两部分，统计每层能产生的白格段，最后用贪心计算答案就可以了，这样最后复杂度就控制在$O(nlogn)$\nCF2056D Unique Median 2200 给了一个整数数组b，当排序后满足$b⌊\\frac{m+1}{2}⌋=b⌈\\frac{m+1}{2}⌉$时我们说数组是好的，题目说给一个长度为n的数组a，计算好子数组数量\n首先观察好数组定义，我们可以很容易发现奇数长度的数组一定是好数组，所以我们只要看偶数长度的数组就可以了，很困难，想了一段时间，一开始的想法是如果我们要让偶数的数组是好的，那么当且仅当存在某个数$v$，使得在选取的子数组中$v \\ge m/2$，也就是说至少一半都是同一个数，但是肯定会有重复用容斥修正，减掉重复的就可以，实现很麻烦，写了很久，而且还发现是错误的，对于[1,1,2,3]按照这个思路明显错的，而且发现正着处理非常麻烦困难，于是决定反向试试，从所有子数组中减掉\u0026quot;坏的\u0026quot;，再修正\n很快就发现了一些东西，对于一个偶数的数组，只要让排序后第k个数小于k+1个数一定是坏的，再具体点，我们选定两个相邻的数x，y，我们只要让左中位数小于等于x，右中位数大于等于y就可以了，然后用前缀和+map即可，并且小于等于x的数的数量和大于等于y的数字数量必须要一样多，那么我们可以把数组按照值分为3类，一种小于等于x我们记为-1，大于等于y记为+1，介于x和y之间的我们直接清空，因为出现就相当于直接破坏了我们的数组\n最后就是用容斥进行修正，只要x和y相差超过2就记为重复的减掉就可以了\nCF1854A2 Dual (Hard Version) 一道思维题，给了一个数组，可以对于$i,j$的元素进行操作，使得$a_i := a_i + a_j$\n要求在31次操作内让数组不递减\n首先当所有元素都大于等于0或者全部都小于等于0的时候很显然，我们只需要把右边的数依次加上左边的数肯定是对的（小于等于0同理反过来就行）\n所以关键是要看数组里有正有负，我们设最大的正数是mx，绝对值最大的负数是mi，正数数量是cp，负数数量是cm，那么我们有两种选择，一种是都统一成非负，一种是统一成非正，只要符号统一我们就方便了，拿统一非负举例（非正同理），我们其实就是要让最大的正数足够大，能够覆盖所有负数，我们首先计算mx经过几次翻倍可以大于mi，然后让他翻倍，之后让所有的负数都加上这个mx它们都变成正数了，然后我们就判断一下这两种选择哪个更优即可\n","date":"2025-10-09T00:00:00Z","image":"https://lunatide.tech/p/icpc%E6%97%A5%E5%B8%B8%E8%AE%AD%E7%BB%8301/codeforce_%E5%89%AF%E6%9C%AC_hu_f47c20acf40349f0.jpg","permalink":"https://lunatide.tech/p/icpc%E6%97%A5%E5%B8%B8%E8%AE%AD%E7%BB%8301/","title":"ICPC日常训练01"},{"content":"","date":"2025-09-28T00:00:00Z","image":"https://lunatide.tech/p/lab1-datalab/csapp_hu_113ffe23e1ef718.jpg","permalink":"https://lunatide.tech/p/lab1-datalab/","title":"Lab1 DataLab"},{"content":"加法 无符号加法 无符号的加法很简单，和二进制加法一样，只不过要把overflow的删除，并且把结果取模即可\n检测无符号数中加法的溢出\n令 $s = U +_w^u V$ 为无符号整数 $U$ 和 $V$ 的和，那么当且仅当 $s \u0026lt; U$（或等价的 $s \u0026lt; V$）时产生溢出，这是因为：\n$$ \\begin{aligned} s \u0026= U + V - 2^w \u003c U \\\\ s \u0026= U + V - 2^w \u003c V \\end{aligned} $$补码加法 ![](截屏2025-09-28 15.57.52.jpg)\n对满足 $-2^{w-1} \\leq x$，$y \\leq 2^{w-1}-1$ 的整数 $x$ 和 $y$，有：\n$$ x +_w^t y = \\begin{cases} x + y - 2^w, \u0026 2^{w-1} \\leq x + y \\quad \\text{正溢出} \\\\ x + y, \u0026 -2^{w-1} \\leq x + y \u003c 2^{w-1} \\quad \\text{正常} \\\\ x + y + 2^w, \u0026 x + y \u003c -2^{w-1} \\quad \\text{负溢出} \\end{cases} $$ 两个数的$w$位补码之和与无符号之和有完全相同的位级表示\n利用左移做乘法\n大多数机器中左移比乘法快 编译器会自动生成这样的代码 方法的证明：\n假设 $x$ 的 $w$ 位二进制表示为 $\\left[x_{w-1}, x_{w-2}, \\cdots, x_{0}\\right]$，那么 $\\left[x_{w-1}, x_{w-2}, \\cdots, x_{0}, 0, \\cdots, 0\\right]$ 给出了 $x2^k$ 的 $w+k$ 位二进制表示：\n$$ \\begin{aligned} B2U_{w+k}\\left(\\left[x_{w-1}, x_{w-2}, \\cdots, x_{0}, 0, \\cdots, 0\\right]\\right) \u0026= \\sum_{i=0}^{w-1} x_{i} 2^{i+k} \\\\ \u0026= \\left[\\sum_{i=0}^{w-1} x_{i} 2^{i}\\right] \\cdot 2^{k} \\\\ \u0026= x 2^{k} \\end{aligned} $$对于固定长度的表示，高 $k$ 位被丢弃，左移 $k$ 位的二进制表示为 $$ \\left[x_{w-k-1}, x_{w-k-2}, \\cdots, x_{0}, 0, \\cdots, 0\\right] $$所以 $$ \\begin{aligned} B2U_{w}\\left(\\left[x_{w-k-1}, x_{w-k-2}, \\cdots, x_{0}, 0, \\cdots, 0\\right]\\right) \u0026= \\sum_{i=0}^{w-k-1} x_{i} 2^{i+k} \\\\ \u0026= \\left[\\sum_{i=0}^{w-k-1} x_{i} 2^{i}\\right] \\cdot 2^{k} \\\\ \u0026= \\left[\\sum_{i=0}^{w-1} x_{i} 2^{i}\\right] \\cdot 2^{k} \\mod 2^w \\\\ \u0026= x2^k \\mod 2^w \\end{aligned} $$所以对于无符号整数 $$ B2U_{w}\\left(\\left[x_{w-k-1}, x_{w-k-2}, \\cdots, x_{0}, 0, \\cdots, 0\\right]\\right) = \\text{UMult}_w(x,2^k) $$对于有符号整数，利用 $$ \\text{TMult}_w(u,v) = U2T_w((u \\cdot v) \\mod 2^w) $$ 可以得到相同的结果。\n一般情形\n现在考虑一般的情形，假设我们需要计算 $u \\times K$，其中 $K$ 为常数，将 $K$ 表达为一组 $0$ 和 $1$ 交替的序列 $$ [(0 \\cdots 0)(1 \\cdots 1)(0 \\cdots 0) \\cdots (1 \\cdots 1)] $$考虑一组从位置 $n$ 到位置 $m$ 的连续 $1$，那么可以用如下方式计算这部分的对于乘积的影响： $$ \\begin{aligned} \u0026(x \u003c\u003c n) + (x \u003c\u003c (n-1)) + \\cdots + (x \u003c\u003c m) \\\\ \u0026(x \u003c\u003c (n+1)) - (x \u003c\u003c m) \\end{aligned} $$移位操作和二进制乘除法的联系 使用移位操作代替除以 2 的幂（无符号） $u \u0026raquo; k$ 给出 $\\lfloor u / 2^{k} \\rfloor$ 使用逻辑移位 证明：\n假设 $x$ 的 $w$ 位二进制表示为 $\\left[x_{w-1}, x_{w-2}, \\cdots, x_{0}\\right]$，右移 $k$ 位的二进制表示为 $\\left[0, \\cdots, 0, x_{w-1}, x_{w-2}, \\cdots, x_{k}\\right]$\n$$ \\begin{aligned} B2U_w(\\left[0, \\cdots, 0, x_{w-1}, x_{w-2}, \\cdots, x_{k}\\right]) \u0026= \\sum_{i=0}^{w-k-1} x_{i+k} 2^{i} \\\\ B2U_{w}\\left(\\left[x_{w-1}, x_{w-2}, \\cdots, x_{0}\\right]\\right) \u0026= \\sum_{i=0}^{w-1} x_{i} 2^{i} \\\\ \u0026= 2^k \\sum_{i=0}^{w-k-1} x_{i+k} 2^{i} + \\sum_{i=0}^{k-1} x_{i} 2^{i} \\\\ \u0026= 2^k B2U_w(\\left[0, \\cdots, 0, x_{w-1}, x_{w-2}, \\cdots, x_{k}\\right]) + \\sum_{i=0}^{k-1} x_{i} 2^{i} \\end{aligned} $$所以 $$ \\lfloor x / 2^{k} \\rfloor = x \u003e\u003e k $$使用移位操作代替除以 2 的幂（有符号） $u \u0026raquo; k$ 给出 $\\lfloor u / 2^{k} \\rfloor$ 使用算数移位 当 $u \u0026gt; 0$ 时上述方法和无符号情形一致，但是当 $u \u0026lt; 0$ 时则会产生有问题的结果，例如取 $u = -12340$：\n考虑 $k=4,8$ 时的结果，在 C 语言中实际结果为 $-771$ 和 $-48$，之所以和 C 语言中的结果不同，是因为上述算法朝着离 $0$ 更远的方向舍入，所以当 $u \u0026lt; 0$ 时要向上舍入，达到上述效果利用如下事实即可 $$ \\lceil x / y \\rceil = \\lfloor (x + y - 1) / y \\rfloor $$Unsigned 必须做一个明确的分配而不是暗示\n容易犯错误，例如下面的代码会出现无限循环\n1 2 3 unsigend i; for(i = cnt - 2;i \u0026gt;= 2;i --) a[i] += a[i + 1]; 可能会变的很诡异\n1 2 3 #define DELTA sizeof(int)//默认int的size是一个unsigned的size_t int i; for(i = CNT;i - DELTA \u0026gt;= 0;i -=DELTA)//和上面的代码出现一样的问题 正确的代码如下\n1 2 3 size_t i; for (i = cnt-2; i \u0026lt; cnt; i--) a[i] += a[i+1]; 什么时候用无符号整数\n执行模块化算术时使用\n多精度算术 在使用位表示集时使用\n逻辑右移，无符号扩展 （最好不要用unsigned）\n内存中数字的一些底层表示 Word Size 硬件本身并不一定定义字长大小\n任何给定的计算机都具有字长\n直到现在，大部份机器都采用32 bits(4 bytes)作为字长\n地址限制为4 GB($2^{32}$bytes) 越来越多机器具有64位字长\n机器仍然支持多种数据格式\n字长的分数或整数倍 但总是整数比特 比特顺序 分为大端法和小端法\nExample 变量x的value是0x01234567 地址是0x100 ","date":"2025-09-26T00:00:00Z","image":"https://lunatide.tech/p/lecture03-bitsbytes-and-integer-cout/csappp_hu_d6260cc969558569.jpg","permalink":"https://lunatide.tech/p/lecture03-bitsbytes-and-integer-cout/","title":"lecture03 Bits,Bytes and Integer cout"},{"content":"Bits 为什么要用比特\n易于用双稳态元件存储 能在有噪声且不精确的电线上可靠传输 我们可以用二进制来表示浮点数。在一个二进制的小数中：\n小数点左边的第一位权重为 2^0，向左依次为 2^1、2^2、… 小数点右边的第一位权重为 2^-1，向右依次为 2^-2、2^-3、… 因此，如果将数字写成 32 位或者 64 位的字符串会很麻烦。通常将 4 bits 为一组，用十六进制表示。这样十六进制与二进制的转换就非常方便。\n字节\n一个字节等于 8 比特 布尔代数 比特之间的关系（操作） 1 代表 true，0 代表 false Example：表示与操作集合\n表示方式： 一个长度为 $w $的位向量表示集合 ${0, 1, …, w-1}$ 的子集 如果 $a_j = 1，j ∈ A$ for example：{0, 3, 5, 6} 01101001 76543210 对于上面的这个例子，“01101001”代表的就是 “76543210” 中各个位的数字在集合中是否出现，出现即为 1，反之为 0。\n对比逻辑运算符 \u0026amp;\u0026amp;、||、 ! Example：\n!0x41 → 0x00 !0x99 → 0x01 !!ox41 → 0x01 0x69 \u0026amp;\u0026amp; 0x55 → 0x01 p \u0026amp;\u0026amp; *p（避免空指针访问，如果 p 是 NULL，就不会解引用） 移位操作 左移：x \u0026laquo; y 向左移动位次，多余的bits扔掉，添加相对应数目的0 右移：x \u0026raquo; y 逻辑右移\n和左移类似 算数右移\n若第一个数为1，把原本填充的0改为填充1 整数编码 无符号数\n$B2U(X) = \\sum_{i = 0}^{w-1} x_i \\cdot 2^i $\n补码(Two\u0026rsquo;s Complement)\n$B2T(X) = -x_{w-1} \\cdot 2^{w-1} + \\sum_{i=0}^{w-2} x_i \\cdot 2^i$\n数值范围\nW = 16\nValue Type Formula / Binary Pattern Decimal Hex Binary Unsigned Values UMin 0 0 0x0000 00000000 00000000 UMax 2W - 1 65535 0xFFFF 11111111 11111111 Two\u0026rsquo;s Complement Values TMin -2W-1 / 100\u0026hellip;0 -32768 0x8000 10000000 00000000 TMax 2W-1 - 1 / 011\u0026hellip;1 32767 0x7FFF 01111111 11111111 Other Values -1 111\u0026hellip;1 -1 0xFFFF 11111111 11111111 0 000\u0026hellip;0 0 0x0000 00000000 00000000 因此我们可以得到一个关系如下：\n​\t$TMin = TMax + 1$\n​\t$UMax = 2 \\times TMax + 1$\n转换 有符号整型和无符号整型和十进制的转换关系如下：\n显然上述转换是可逆的：\n$\\text{U2B}(x)=\\text{B2U}^{-1}(x)$ $\\text{T2B}(x)=\\text{B2T}^{-1}(x)$ 利用复合关系可以得到有符号整型以及无符号整型的转换关系：\n对于$\\text {TMin} \\leqslant x \\leqslant \\text{TMax}$ $$ T 2 U_w(x)=\\left\\{\\begin{array}{ll} x+2^{w}, \u0026 x\u003c0 \\\\ x, \u0026 x \\geqslant 0 \\end{array}\\right. $$ 对于$0 \\leqslant x \\leqslant \\text{UMax}$ $$ U 2 T_{w}(u)=\\left\\{\\begin{array}{ll} u, \u0026 u \\leqslant \\text{TMax} \\\\ u-2^{w}, \u0026 u\u003e\\text{TMax} \\end{array}\\right. $$下图为补码到无符号数的转换关系\nC语言中的转换如下：\n1 2 3 4 5 6 7 8 9 int tx, ty; unsigned ux, uy; //显示转换 tx = (int) ux; uy = (unsigned) ty; //隐式转换 tx = ux; uy = ty 表达式求值规则 若单个表达式中同时出现无符号数与有符号数，则有符号值会被隐式转换为无符号数（包括比较运算 \u0026lt;、\u0026gt;、==、\u0026lt;=、\u0026gt;=）。\n举例：W = 32 位\n常量定义\nTMIN = -2,147,483,648 TMAX = 2,147,483,647 常量 1 常量 2 实际类型 关系 结果 说明 0 0U 无符号 \u0026lt; 假 两者均为 0 -1 0 有符号 \u0026lt; 真 普通有符号比较 -1 0U 无符号 \u0026gt; 真 -1 被转成 4294967295U 2147483647 -2147483648 有符号 \u0026gt; 真 普通有符号比较 2147483647U -2147483648 无符号 \u0026lt; 真 -2147483648 被转成 2147483648U -1 -2 有符号 \u0026gt; 真 普通有符号比较 (unsigned)-1 -2 无符号 \u0026gt; 真 -2 被转成 4294967294U 2147483647 2147483648U 无符号 \u0026lt; 真 前者 \u0026lt; 后者 2147483647 (int)2147483648U 有符号 \u0026gt; 真 后者溢出成 -2147483648 小结：有符号与无符号强制转换的基本规则 位模式保持不变\n强制转换时，内存中的 0/1 序列原样复制。\n仅重新解释\n同一段位模式，按目标类型（有符号或无符号）重新解读。\n可能产生“意外”\n数值可能突然加上或减去 2^W（W 为位数）。\n表达式中的混合类型\n只要表达式里同时出现signed int与unsigned int ，编译器会把 signed int 隐式转换成 unsigned int，再参与运算。\n扩展，截断 扩展 无符号扩展至需要在前面的bits补0,有符号扩展如下\n任务：\n给定w-bit的有符号整型x 将其转换为w+k-bit的有符号整型，值不变 规则：\n将符号位复制k份\n$X\u0026rsquo; = X_{w-1},\\dots,X_{w-1},X_{w-1},X_{w-2},\\dots,X_0$\n补码的符号扩展\n定义宽度为 $w$ 的位向量 $\\vec{x} = [x_{w-1}, x_{w-2}, \\dots, x_0]$ 和宽度为 $w\u0026rsquo;$ 的位向量 $\\vec{x}\u0026rsquo; = [x\u0026rsquo;{w\u0026rsquo;-1}, x\u0026rsquo;{w\u0026rsquo;-2}, \\dots, x\u0026rsquo;0]$，其中 $w\u0026rsquo; \u0026gt; w$。则$B2T_w(\\vec{x}) = B2T{w\u0026rsquo;}(\\vec{x}\u0026rsquo;)$，\n补码数值的符号扩展\n令$w\u0026rsquo; = w + k$，我们想要证明的是 $$ B2T_{w+k} ([x_{w-1}, \\cdots, x_{w-1}, x_{w-2}, \\cdots, x_0]) = B2T_w([x_{w-1}, x_{w-2}, \\cdots, x_0]) $$下面的证明是对$k$进行归纳。也就是说，如果我们能够证明符号扩展一位保持了数值不变，那么符号扩展任意位都能保持这种属性。因此，证明的任务就变为了： $$ B2T_{w+1} ([x_{w-1}, x_{w-1}, x_{w-2}, \\cdots, x_0]) = B2T_w([x_{w-1}, x_{w-2}, \\cdots, x_0]) $$展开左边的表达式，得到： $$ \\begin{aligned} B2T_{w+1} ([x_{w-1}, x_{w-1}, x_{w-2}, \\cdots, x_0]) \u0026= -x_{w-1}2^w + \\sum_{i=0}^{w-1}x_i2^i \\\\ \u0026= -x_{w-1}2^w + x_{w-1}2^{w-1} + \\sum_{i=0}^{w-2}x_i2^i \\\\ \u0026= -x_{w-1}(2^w - 2^{w-1}) + \\sum_{i=0}^{w-2}x_i2^i \\\\ \u0026= -x_{w-1}2^{w-1} + \\sum_{i=0}^{w-2}x_i2^i \\\\ \u0026= B2T_w([x_{w-1}, x_{w-2}, \\cdots, x_0]) \\end{aligned} $$我们使用的关键属性是$2^w - 2^{w-1} = 2^{w-1}$。因此，加上一个权值为$-2^w$的位，和将一个权值为 $-2^{w-1}$的位转换为一个权值为 $2^{w-1}$ 的位，这两项运算的综合效果就会保持原始的数值。\n截断 无符号截断\n原始 $w$ 位：\n$$ \\mathrm{B2U}_w(X)=\\sum_{i=0}^{w-1} x_i\\cdot2^i $$截断为 $k$ 位后：\n$$ \\mathrm{B2U}_k(X)=\\sum_{i=0}^{k-1} x_i\\cdot2^i = \\mathrm{B2U}_w(X)\\bmod 2^k $$推导 $$ \\begin{aligned} B2U_w([x_{w-1}, x_{w-2}, \\cdots, x_0]) \\mod 2^k \u0026= \\left[ \\sum_{i=0}^{w-1} x_i 2^i \\right] \\mod 2^k \\\\ \u0026= \\left[ \\sum_{i=0}^{k-1} x_i 2^i \\right] \\mod 2^k \\\\ \u0026= \\sum_{i=0}^{k-1} x_i 2^i \\\\ \u0026= B2U_k([x_{k-1}, x_{k-2}, \\cdots, x_0]) \\end{aligned} $$有符号截断\n原始 $w$ 位：\n$$ \\mathrm{B2T}_w(X)=-x_{w-1}\\cdot2^{w-1}+\\sum_{i=0}^{w-2} x_i\\cdot2^i $$ 截断为 $k$ 位后：\n$$ \\mathrm{B2T}_k(X)=-x_{k-1}\\cdot2^{k-1}+\\sum_{i=0}^{k-2} x_i\\cdot2^i = \\mathrm{U2T}_k\\!\\bigl(\\mathrm{B2U}_w(X)\\bmod 2^k\\bigr) $$","date":"2025-09-24T00:00:00Z","image":"https://lunatide.tech/p/lecture02-bitsbytes-and-integer/csapp_hu_d6260cc969558569.jpg","permalink":"https://lunatide.tech/p/lecture02-bitsbytes-and-integer/","title":"lecture02 Bits,Bytes and Integer"}]