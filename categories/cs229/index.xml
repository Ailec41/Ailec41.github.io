<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CS229 on LunaTide's Blog</title><link>https://lunatide.tech/categories/cs229/</link><description>Recent content in CS229 on LunaTide's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>LunaTide's Blog</copyright><lastBuildDate>Fri, 02 Jan 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://lunatide.tech/categories/cs229/index.xml" rel="self" type="application/rss+xml"/><item><title>CS229 Lecture 5</title><link>https://lunatide.tech/p/cs229-lecture-5/</link><pubDate>Fri, 02 Jan 2026 00:00:00 +0000</pubDate><guid>https://lunatide.tech/p/cs229-lecture-5/</guid><description>&lt;img src="https://lunatide.tech/p/cs229-lecture-5/pic1.jpg" alt="Featured image of post CS229 Lecture 5" /&gt;</description></item><item><title>CS229 Lecture 4</title><link>https://lunatide.tech/p/cs229-lecture-4/</link><pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate><guid>https://lunatide.tech/p/cs229-lecture-4/</guid><description>&lt;img src="https://lunatide.tech/p/cs229-lecture-4/pic1.jpg" alt="Featured image of post CS229 Lecture 4" /&gt;&lt;p&gt;现在是2026年01月01号的00:00，该更新note了！新年快乐！&lt;/p&gt;
&lt;h3 id="另外一种最大化ltheta的算法"&gt;另外一种最大化$l(\theta)$的算法
&lt;/h3&gt;&lt;p&gt;回到logistic回归，其中$g(z)$是sigmoid函数，现在让我们讨论一种最大化$l(\theta)$的不同算法&lt;/p&gt;
&lt;p&gt;我们先想一下求一个方程零点的牛顿法。假设我们有一个从实数到实数$ f : \mathbb{R} \to \mathbb{R} $，然后要找到一个$\theta$，来满足 $f(\theta)=0$，其中$0 \in \mathbb{R}$。牛顿法就是对 $\theta$ 进行如下的更新：
&lt;/p&gt;
$$
\theta := \theta - \frac{f(\theta)}{f'(\theta)}\tag{1}
$$&lt;p&gt;
这个方法可以通过一个很自然的解释，我们可以把它理解成用一个线性函数来对函数 $f$ 进行逼近，这条直线是 $f$ 的切线，求解线性函数等于0的位置，并让下一个$\theta$ 的猜测为线性函数为0的地方&lt;/p&gt;
&lt;p&gt;下面是牛顿法的图解&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lunatide.tech/p/cs229-lecture-4/p1.jpg"
width="887"
height="251"
srcset="https://lunatide.tech/p/cs229-lecture-4/p1_hu_6f45aff28d8f90d3.jpg 480w, https://lunatide.tech/p/cs229-lecture-4/p1_hu_14fecb85cb6da5d2.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="353"
data-flex-basis="848px"
&gt;&lt;/p&gt;
&lt;p&gt;在最左边的图中，我们可以看到函数 $f$ 就是沿着 $y = 0$ 的一条直线。这时候是想要找一个 $\theta$ 来让函数值等于0，这时候发现这个 $\theta$ 值大概在1.3左右。假设我们设定初始值$\theta=4.5$。牛顿法就是在$\theta=4.5$的地方画一条切线（中间的图，这样就给出了下一个 $\theta$ 猜测值的地方，也就是这个切线的零点，大概是2.8。最右面的图中是再次进行一次这样的迭代后的结果，这时候的 $\theta$ 大概为1.8。这样多次迭代过后，很快就能接近 $\theta=1.3$&lt;/p&gt;
&lt;p&gt;牛顿方法给出了一种求解$f(\theta)=0$的方法。如果我们想用它来最大化某些函数 $l$ 该怎么做？$l$ 的最大值对应于其一阶导数 $l&amp;rsquo;(\theta)$ 为零的点。因此，通过令 $f(\theta)=l&amp;rsquo;(\theta)$，我们可以用相同的算法来最大化 $l$，并且我们获得更新规则：
&lt;/p&gt;
$$
\theta := \theta-\frac{l'(\theta)}{l''(\theta)}\tag{2}
$$&lt;p&gt;
（最小化 $l$ 也是使用上述更新规则）&lt;/p&gt;
&lt;p&gt;最后，在我们的logistic回归背景中，$\theta$ 是一个有值的向量，所以我们要对牛顿法进行扩展来适应这个情况。牛顿法进行扩展到多维情况，也叫牛顿-拉普森法（Newton-Raphson method）吗，如下
&lt;/p&gt;
$$
\theta := \theta - H^{-1} \nabla_\theta \ell(\theta)\tag{3}
$$&lt;p&gt;
在这里，\( \nabla \ell(\theta) \) 和往常一样，是 \( \ell(\theta) \) 关于 \( \theta_i \) 的偏导数的向量。而 \(H\) 是一个 \( n \times n \) 矩阵（实际上是\( (n+1) \times (n+1) \)，假设我们包括截距项），称为 Hessian。矩阵的每一项由下式给出：&lt;/p&gt;
$$
H_{ij} = \frac{\partial^2 \ell(\theta)}{\partial \theta_i \, \partial \theta_j}\tag{4}
$$&lt;p&gt;
牛顿法通常都能比（批量）梯度下降法收敛得更快，而且达到最小值所需要的迭代次数也低很多。然而，牛顿法中的单次迭代往往要比梯度下降法的单步耗费更多的性能开销，因为要查找和转换一个 \( n \times n \) 的Hessian矩阵；不过只要这个n不是太大，牛顿法通常还是更快一些。只用牛顿法来logistic回归中求似然函数 $l(\theta)$ 的最大值的时候，得到这个结果的方法叫做&lt;strong&gt;Fisher积分&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="广义线性模型"&gt;广义线性模型
&lt;/h2&gt;&lt;p&gt;到目前为止，我们已经看到了回归样例和分类样例。在回归样例中，我们有 \( y \mid x; \theta \sim \mathcal{N}(\mu, \sigma^2) \)， 在分类中， \( y \mid x; \theta \sim \text{Bernoulli}(\phi) \)， \( \mu \) 和 \( \phi \) 定义为 \( x \) 和 \(\theta\) 的函数。在本节中，我们将展示这两种方法都是更广泛的模型族的特例，称为&lt;strong&gt;广义线性模型（GLM）&lt;/strong&gt;。我们还将展示如何推导 GLM 族中的其他模型，并将其应用于其他分类和回归问题。&lt;/p&gt;
&lt;h3 id="指数族"&gt;指数族
&lt;/h3&gt;&lt;p&gt;为了完成GLM，我们将首先定义指数族分布。我们说一类分布属于指数族，如果它们可以表达为如下形式：
&lt;/p&gt;
$$
p(y; \eta) = b(y)\,\exp\!\big(\eta^{T} T(y) - a(\eta)\big)\tag{5}
$$&lt;p&gt;
在这里，$\eta$ 称为分布的&lt;strong&gt;自然参数&lt;/strong&gt;（也称为&lt;strong&gt;规范参数&lt;/strong&gt;）；$T(y)$是&lt;strong&gt;充分统计量&lt;/strong&gt;，我们目前用的这些分布中通常 $T(y)=y$；而$a(\eta)$是一个&lt;strong&gt;对数分割函数&lt;/strong&gt;。$e^{-a(\eta)}$这个量本质上扮演了归一化常数的角色，也就是确保$p(y;\eta)$的综合或者积分等于1&lt;/p&gt;
&lt;p&gt;定义分布族的$T,a,b$由$\eta$参数化；当我们改变$\eta$的时候，我们在这个族中会获得不同的分布&lt;/p&gt;
&lt;p&gt;现在我们看到的伯努利分布和高斯分布就都属于指数分布族。伯努利分布的均值是$ \phi $，也写作Bernoulli($ \phi $)，确定的分布是$y \in {0,1}$，因此有 $p(y=1;\phi) = \phi;p(y=0;\phi)=1-\phi$。这时候只要修改$ \phi $，我们获得不同均值的伯努利分布。我们现在表明，这类伯努利分布，即通过变化$ \phi $获得的分布，是指数族；即，可以选择$T,a,b$，使得等式(5)恰好成为伯努利分布&lt;/p&gt;
&lt;p&gt;我们现在将伯努利分布改写：
&lt;/p&gt;
$$
\begin{aligned}
p(y; \eta)
&amp;= \phi^{y}(1-\phi)^{1-y} \\
&amp;= \exp\!\big(y \log \phi + (1-y)\log(1-\phi)\big) \\
&amp;= \exp\!\left(\left(\log\frac{\phi}{1-\phi}\right)y + \log(1-\phi)\right)
\end{aligned}
$$&lt;p&gt;
因此，自然参数由$ \phi=log(\phi/(1-\phi)) $给出，有趣的是，如果我们翻转这个定义，用$\eta$来解 $\phi$ 就会得到 $\phi=1/(1+e^{-\eta})$。这正好就是之前我们刚刚见过的sigmoid函数！在我们把logistic回归作为一种广义线性模型（GLM）时还会遇到这个情况。
&lt;/p&gt;
$$
\begin{aligned}
T(y) &amp;= y \\
a(\eta) &amp;= -\log(1-\phi) \\
&amp;= \log(1 + e^{\eta}) \\
b(y) &amp;= 1
\end{aligned}
$$&lt;p&gt;
上面的这组式子就表明了伯努利分布可以写成等式(6)的形式，使用一组合适的$T,a,b$&lt;/p&gt;
&lt;p&gt;接下来继续看高斯分布。回顾一下，当推导出线性回归的时候，\( \sigma^2 \) 的值对 \(\theta\) 和 \( h(x) \) 的最终选择没有影响。因此，我们可以选择任意值的 \( \sigma^2 \) 而不改变任何东西。为了简化下面的推导，让我们令 \( \sigma^2 = 1 \)，然后我们有：
&lt;/p&gt;
$$
\begin{aligned}
p(y; \mu)
&amp;= \frac{1}{\sqrt{2\pi}}
\exp\!\left(-\frac{1}{2}(y-\mu)^2\right) \\
&amp;= \frac{1}{\sqrt{2\pi}}
\exp\!\left(-\frac{1}{2}y^2\right)
\cdot
\exp\!\left(\mu y - \frac{1}{2}\mu^2\right)
\end{aligned}
$$&lt;p&gt;因此，我们看到高斯分布是指数族，并且有：&lt;/p&gt;
$$
\begin{aligned}
\eta &amp;= \mu \\
T(y) &amp;= y \\
a(\eta) &amp;= mu^2/2 \\
&amp;= \eta^2/2 \\
b(y) &amp;= (1/\sqrt{2\pi}) \exp\!\left(-y^2/2\right)
\end{aligned}
$$&lt;p&gt;
指数分布族里面还有很多其他的分布，例如多项式分布，这个待会就能看到；泊松分布，用来对计数类数据进行建模；伽马和指数分布，用于对连续的、非负的随机变量进行建 模，例如时间间隔；$\beta$ 和狄利克雷分布，用于概率的分布；还有很多，在下一节我们会描述建模型的一般方法，其中$y$（给定$ x $和$\theta$）来自任何这些分布&lt;/p&gt;</description></item><item><title>CS229 Lecture 3</title><link>https://lunatide.tech/p/cs229-lecture-3/</link><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate><guid>https://lunatide.tech/p/cs229-lecture-3/</guid><description>&lt;img src="https://lunatide.tech/p/cs229-lecture-3/pic1.jpg" alt="Featured image of post CS229 Lecture 3" /&gt;&lt;h2 id="最小二乘法的概率解释"&gt;最小二乘法的概率解释
&lt;/h2&gt;&lt;p&gt;在面对回归问题的时候，我们会思考，为什么选择线性回归，为什么选择最小二乘法成本函数 &lt;strong&gt;J&lt;/strong&gt; ？在本节里会给出一系列的概率基本假设，基于这些假设，可以推出最小二乘法是一种非常自然的算法&lt;/p&gt;
&lt;p&gt;首先假设目标变量和输入值存在下面这种等量关系
&lt;/p&gt;
$$
y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}
$$&lt;p&gt;
上式中的 $\epsilon^{(i)}$ 是误差项，用于存放由于建模所忽略的变量导致的效果或者随机的噪音信息。进一步假设$\epsilon^{(i)}$是独立同分布的（IID），服从高斯分布，其平均值为0，方差为$\sigma^2$，这样就可以把这个假设写成&amp;quot;$\epsilon^{(i)} \sim N(0,\sigma^2)$&amp;quot;，然后$\epsilon^{(i)}$的密度函数就是：
&lt;/p&gt;
$$
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(\epsilon^{(i)})^2}{2\sigma^2} \right)
$$&lt;p&gt;这意味着存在下面的等量关系：&lt;/p&gt;
$$
p(y^{(i)} \mid x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right)
$$&lt;p&gt;
这里的记号 &amp;ldquo;$p(y^{(i)} \mid x^{(i)}; \theta)$&amp;ldquo;表示的是这是一个对于给定$x^{(i)}$的$y^{(i)}$的分布，用$\theta$进行了参数化，这里不能用&amp;rdquo;$p(y^{(i)} \mid x^{(i)}, \theta)$&amp;ldquo;来当作条件，因为$\theta$并不是一个随机变量，也可以将$y^{(i)}$的分布写成$y^{(i)} \mid x^{(i)};\theta \sim N(\theta^Tx^{(i)},\sigma^2)$&lt;/p&gt;
&lt;p&gt;给定一个 $X$ 为设计矩阵，包含了全部$x^{(i)}$，然后再给定$\theta$，那么$y^{(i)}$的分布是什么？数据的概率由$p(\vec{y} \mid X;\theta)$的形式给出。在$\theta$取某个固定值的情况下，这个等式通常可以看作一个$\vec{y}$的函数。当我们把它当作 $\theta$ 的函数时，就称它为似然函数
&lt;/p&gt;
$$
L(\theta)=L(\theta;X,\vec{y})= p(\vec{y} \mid X;\theta)
$$&lt;p&gt;
结合之前对 $\epsilon^{(i)}$ 的独立性假设(这里对$y^{(i)}$ 以及给定的$x^{(i)}$也都做同样假设)，就可以把上面这个等式改写成下面的形式
&lt;/p&gt;
$$
\begin{aligned}
L(\theta) &amp;= \prod_{i=1}^{m} p(y^{(i)} \mid x^{(i)}; \theta) \\
&amp;= \prod_{i=1}^{m} \left[ \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right) \right]
\end{aligned}
$$&lt;p&gt;
现在，给定了 $y^{(i)}$ 和 $x^{(i)}$ 之间关系的概率模型了，用什么方法来选择咱们对参数 $\theta$ 的最佳猜测呢，最大似然法告诉我们要选择能让数据的似然函数尽可能大的 $\theta$ 。也就是说，咱们要找的 $\theta$ 能够让函数 $L(\theta)$ 取到最大值&lt;/p&gt;
&lt;p&gt;为了运算方便，实际中我们选择最大化对数似然函数$l(\theta)$:
&lt;/p&gt;
$$
\begin{aligned}
\ell(\theta) &amp;= \log L(\theta) \\
&amp;= \log \prod_{i=1}^{m} \left[ \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right) \right] \\
&amp;= \sum_{i=1}^{m} \log \left[ \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right) \right] \\
&amp;= m \log \frac{1}{\sqrt{2\pi}\sigma}
\;-\; \frac{1}{2\sigma^2} \sum_{i=1}^{m} (y^{(i)} - \theta^T x^{(i)})^2
\end{aligned}
$$&lt;p&gt;
因此，对$l(\theta)$的最大值也就意味着下面这个子式取到最小值
&lt;/p&gt;
$$
\frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - \theta^T x^{(i)})^2
$$&lt;p&gt;
上式即为$J(\theta)$，我们最初的最小二乘成本函数&lt;/p&gt;
&lt;p&gt;总结：在对数据进行概率假设的基础上，最小二乘回归得到的 $\theta$ 和最大似然法估计的 $\theta$ 是一致的。所以这是一系列的假设，其前提是认为最小二乘回归能够被判定为一种非常自然的方法，这种方法正好就进行了最大似然估计&lt;/p&gt;
&lt;p&gt;还要注意，在刚才的讨论中，我们最终对 $\theta$ 的选择并不依赖 $\sigma^2$ ，而且也确实在不知道 $\sigma^2$ 的情况下就找到了结果。&lt;/p&gt;
&lt;h2 id="局部加权线性回归"&gt;局部加权线性回归
&lt;/h2&gt;&lt;p&gt;假如问题从 $x \in R$来预测 y。下面第一幅图显示了使用 $y=\theta_0+\theta_1x$来对一个数据集来进行拟合。我们明显能看出来这个数据的趋势不是一条严格的直线，所以用直线进行的拟合就不是好的方法。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lunatide.tech/p/cs229-lecture-3/p1.jpg"
width="1494"
height="430"
srcset="https://lunatide.tech/p/cs229-lecture-3/p1_hu_451db920bd6b617.jpg 480w, https://lunatide.tech/p/cs229-lecture-3/p1_hu_4b4f8c0d802f27a8.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="347"
data-flex-basis="833px"
&gt;&lt;/p&gt;
&lt;p&gt;那要是我们添加一个二次项，用 $y=\theta_0 + \theta_1x + \theta_2x^2$来拟合（上面中间的图），明显如果我们对特征补充得越多，效果就越好。不过增加太多的特征也会造成危险，看第三张图就是拟合5项多项式$y=\sum_{j=0}^{5}\theta_jx^j$的结果，可以看到，虽然拟合曲线完美地通过了所有当前数据集中的数据，但我们明显不能认为这个曲线是一个合适的预测工具，比如针对不同的居住面积 $x$ 来预测房屋价格 $y$ ，左边的图是一个&lt;strong&gt;欠拟合&lt;/strong&gt;的例子，明显看到漏掉了数据集中的结构信息，而最右边的图是&lt;strong&gt;过拟合&lt;/strong&gt;的例子&lt;/p&gt;
&lt;p&gt;因此，如上面例子所示，特征的选择对于确保学习算法的良好性能很重要，在本节，我们会简单地谈谈局部加权线性回归算法，这里假设有足够的训练数据，使得特征选择不那么重要&lt;/p&gt;
&lt;p&gt;在原始版本的线性回归算法中，要对一个查询点 $x$ 进行预测，比如要衡量 $h(x)$，要经过下面的步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用参数 $\theta$ 进行拟合，让数据集中的值与拟合算出的值差值平方 $(y^{(i)} - \theta^Tx^{(i)})^2$最小（最小二乘法的思想）&lt;/li&gt;
&lt;li&gt;输出 $\theta^Tx$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相应地，在 LWR 局部加权线性回归的方法中，步骤如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用参数 $\theta$ 进行拟合，让加权距离$w^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$最小&lt;/li&gt;
&lt;li&gt;输出$\theta^Tx$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面式子中的 $w^{(i)}$是非负的权值，直观地，如果$w^{(i)}$对于特定的 $i$ 很大，那么在选择 $\theta$ 时，我们将努力使 $(y^{(i)}-\theta^Tx^{(i)})$变小。如果$w^{(i)}$很小，则拟合中几乎忽略了 $(y^{(i)}-\theta^Tx^{(i)})^2$ 误差项&lt;/p&gt;
&lt;p&gt;对于权值的选择可以使用下面这个比较标准的公式：
&lt;/p&gt;
$$
w^{(i)}=\exp(- \frac{(x^{(i)}-x)^2}{2\tau^2} )
$$&lt;p&gt;
参数$\tau$控制训练样本的权重随着其$x^{(i)}$距查询点$x$的距离而下降的速度，$\tau$称为&lt;strong&gt;带宽参数&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;局部加权线性回归是我们看到的&lt;strong&gt;非参数&lt;/strong&gt;算法的第一个例子。我们之前看到的(未加权)线性回归算法被称为&lt;strong&gt;参数&lt;/strong&gt;算法，因为其具有固定的，有限数量的参数($\theta$)，这些参数由数据拟合。一旦我们拟合了$\theta_i$并将它们存储起来，我们就不再需要保留训练数据来做出对未来的预测，相反，如果用局部加权线性回归算法，我们就必须一直保留着整个训练集，这里的&amp;quot;非参数&amp;quot;粗略的指为了呈现出假设h遂着数据集的规模大增长而线性增长，我们需要用一定顺序保存一些数据的规模&lt;/p&gt;
&lt;h2 id="分类与逻辑回归"&gt;分类与逻辑回归
&lt;/h2&gt;&lt;p&gt;分类问题其实和回归问题很像，只不过我们现在要来预测的$y$的值只局限于少数的若干个离散值。首先关注的是二值化分类问题，也就是说咋们要判断的 $y$ 只有两个取值，0或者1（这里说到的大多数内容也将推广到多类情况）。例如，如果我们正在尝试为电子邮件构建垃圾邮件分类器，则 $x^{(i)}$ 可能是电子邮件的某些特征，如果是垃圾邮件，则$y$为1，否则为0。0也称为&lt;strong&gt;负类&lt;/strong&gt;，1表示&lt;strong&gt;正类&lt;/strong&gt;，它们有时也用符号&amp;rdquo;-&amp;ldquo;和&amp;rdquo;+&amp;ldquo;表示，给定$x^{(i)}$，相应的$y^{(i)}$也称为训练样本的&lt;strong&gt;标签&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="logistic回归"&gt;Logistic回归
&lt;/h3&gt;&lt;p&gt;我们可以忽略$y$是离散值的事实来处理分类问题，并使用我们的旧线性回归算法来尝试预测给定的$x$的$y$。但是，很容易构造此方法的效果非常差的例子。直觉上，当我们知道$y \in {0,1}$，所以$h_0(x)$的值如果大于1或者小于0就没有意义了，就是说$y$的值必然应当是0和1这两个值中的一个&lt;/p&gt;
&lt;p&gt;所以咱们就改变一下假设函数$h_0(x)$的形式，来解决这个问题。比如咱们可以选择下面这个函数：
&lt;/p&gt;
$$
h_0(x)=g(\theta^Tx)= \frac{1}{1+e^{-\theta^Tx}}
$$&lt;p&gt;
其中有:
&lt;/p&gt;
$$
g(z) = \frac{1}{1+e^{-z}}
$$&lt;p&gt;
这个函数就是我们所熟悉的sigmoid函数，也叫做logistic函数，下面是它的图像&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lunatide.tech/p/cs229-lecture-3/p2.jpg"
width="976"
height="694"
srcset="https://lunatide.tech/p/cs229-lecture-3/p2_hu_bc222519dbcf63cf.jpg 480w, https://lunatide.tech/p/cs229-lecture-3/p2_hu_388959615ff42ec7.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="337px"
&gt;&lt;/p&gt;
&lt;p&gt;注意到，当 $z \to \infty$ 时 $g(z)$ 趋近于 1，而当 $z \to -\infty$ 时 $g(z)$ 趋近于 0。 此外，$g(z)$ 和 $h(x)$ 的值总是在 0 和 1 之间波动。 我们保持 $x_0 = 1$ 的约定，所以$\theta^T x = \theta_0 + \sum_{j=1}^{n} \theta_j x_j$&lt;/p&gt;
&lt;p&gt;现在咱们就把 g 作为选定的函数了。当然其他的从0到1之间光滑递增的函数也可以使用，不过后面我们会了解到选择g的一些原因，对这个逻辑函数的选择是很自然的。在继续深入之前，在继续深入之前，下面通过导数讲解下这个函数的一些性质
&lt;/p&gt;
$$
\begin{aligned}
g'(z) &amp;= \frac{d}{dz} \frac{1}{1 + e^{-z}} \\
&amp;= \frac{1}{(1 + e^{-z})^{2}} \, (e^{-z}) \\
&amp;= \frac{1}{(1 + e^{-z})} \left( 1 - \frac{1}{1 + e^{-z}} \right) \\
&amp;= g(z)(1 - g(z)).
\end{aligned}
$$&lt;p&gt;
给定了逻辑回归模型了，咱们怎么去拟合一个合适的 $\theta$ 呢？我们之前已经看到了在一系列前提下，最小二乘法回归可以通过最大似然估计来推出，那么接下来就给我们这个分类模型做一系列的统计学假设，然后用最大似然法拟合参数&lt;/p&gt;
&lt;p&gt;首先假设：
&lt;/p&gt;
$$
P(y = 1 \mid x;\theta) = h_\theta(x)
$$$$
P(y = 0 \mid x;\theta) = 1 - h_\theta(x)
$$&lt;p&gt;更简洁的写法是：&lt;/p&gt;
$$
p(y \mid x;\theta) = (h_\theta(x))^{y}\,(1 - h_\theta(x))^{1 - y}
$$&lt;p&gt;
假设m个训练样本都是各自独立生成的，那么就可以按如下的方式来写参数的似然函数：
&lt;/p&gt;
$$
\begin{aligned}
L(\theta) &amp;= p(\vec{y} \mid X; \theta) \\
&amp;= \prod_{i=1}^{m} p(y^{(i)} \mid x^{(i)}; \theta) \\
&amp;= \prod_{i=1}^{m} \left( h_\theta(x^{(i)})^{\,y^{(i)}} \, (1 - h_\theta(x^{(i)}))^{\,1 - y^{(i)}} \right)
\end{aligned}
$$&lt;p&gt;
然后和之前一样，取个对数就很容易计算最大值
&lt;/p&gt;
$$
\begin{aligned}
\ell(\theta) &amp;= \log L(\theta) \\
&amp;= \sum_{i=1}^{m}\Big( y^{(i)} \log h(x^{(i)}) + \big(1 - y^{(i)}\big)\log\big(1 - h(x^{(i)})\big) \Big)
\end{aligned}
$$&lt;p&gt;
怎么让似然函数最大？就跟之前在线性回归的时候用了求导数的方法类似，咱们这次就是用&lt;strong&gt;梯度上升法&lt;/strong&gt;。还是写成向量的形式，然后更新，就是$\theta := \theta + \alpha \nabla_{\theta}\ell(\theta)$。（因为找最大值，所以是加号），还是先从只有一组训练样本(x,y)开始，然后求导数来退出随机梯度上升规则：
&lt;/p&gt;
$$
\begin{aligned}
\frac{\partial}{\partial \theta_j}\ell(\theta)
&amp;= \left( y \frac{1}{g(\theta^T x)} - (1-y)\frac{1}{1-g(\theta^T x)} \right)
\frac{\partial}{\partial \theta_j} g(\theta^T x) \\
&amp;= \left( y \frac{1}{g(\theta^T x)} - (1-y)\frac{1}{1-g(\theta^T x)} \right)
g(\theta^T x)\bigl(1-g(\theta^T x)\bigr)\,
\frac{\partial}{\partial \theta_j}\theta^T x \\
&amp;= \Bigl( y\bigl(1-g(\theta^T x)\bigr) - (1-y)g(\theta^T x) \Bigr)\,x_j \\
&amp;= \bigl( y - h_\theta(x) \bigr)\,x_j
\end{aligned}
$$&lt;p&gt;上面的式子里，我们用到了对函数求导的定理$g&amp;rsquo;(z)=g(z)(1-g(z))$。然后就用到了随机梯度上升规则：
&lt;/p&gt;
$$
\theta_j := \theta_j + \alpha\bigl(y^{(i)}-h_\theta(x^{(i)})\bigr)x_j^{(i)}
$$&lt;p&gt;
如果我们将其和 &lt;strong&gt;LMS&lt;/strong&gt; 更新规则相对比，就能发现看上去挺相似的；&lt;strong&gt;不过这并不是同一个算法&lt;/strong&gt;，因为这里的$h_0(x^{(i)})$现在定义成了一个$\theta^Tx^{(i)}$的非线性函数尽管如此，我们面对不同的学习问题使用了不同的算法，却得到了看上去一样的更新规则，这是巧合吗，我们学到GLM广义线性模型的时候就会得到答案了。&lt;/p&gt;
&lt;h2 id="题外话感知器学习算法"&gt;题外话：感知器学习算法
&lt;/h2&gt;&lt;p&gt;现在简单聊一个算法，它的历史很有趣，并且之后讲学习理论的时候还要讲到它。设想一下，对逻辑回归方法修改一下，“强迫” 它输出的值要么是0要么是1。要实现这个目的，很自然就应该把函数 $g$ 的定义修改一下，改成一个阙值函数
&lt;/p&gt;
$$
g(z)=
\begin{cases}
1, &amp; z \ge 0 \\
0, &amp; z &lt; 0
\end{cases}
$$&lt;p&gt;
若是我们还像之前一样令 $h_\theta(x)=g(\theta^Tx)$，但用刚刚上面的阙值函数作为 $g$ 的定义，然后如果我们用了下面的更新规则：
&lt;/p&gt;
$$
\theta_j := \theta_j + \alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}
$$&lt;p&gt;
这样我们就得到了感知器学习算法&lt;/p&gt;</description></item><item><title>CS229作业0</title><link>https://lunatide.tech/p/cs229%E4%BD%9C%E4%B8%9A0/</link><pubDate>Sat, 06 Dec 2025 00:00:00 +0000</pubDate><guid>https://lunatide.tech/p/cs229%E4%BD%9C%E4%B8%9A0/</guid><description>&lt;img src="https://lunatide.tech/p/cs229%E4%BD%9C%E4%B8%9A0/pic1.jpg" alt="Featured image of post CS229作业0" /&gt;&lt;h2 id="1-gradients-and-hessians"&gt;1. Gradients and Hessians
&lt;/h2&gt;&lt;h3 id="a"&gt;a.
&lt;/h3&gt;&lt;p&gt;由第一项，我们可以得到：&lt;/p&gt;
$$
f_1(x) = \frac{1}{2} x^T A x + b^T x
$$&lt;p&gt;因为：&lt;/p&gt;
$$
\nabla_x(x^T A x) = (A + A^T)x
$$&lt;p&gt;因此：&lt;/p&gt;
$$
\nabla_x\left( \frac{1}{2} x^T A x \right)
= \frac{1}{2}(A + A^T)x
$$&lt;p&gt;因为 \(A\) 是对称矩阵（\(A^T = A\)），所以：&lt;/p&gt;
$$
\frac{1}{2}(A + A)x = Ax
$$&lt;p&gt;第二项：&lt;/p&gt;
$$
f_2(x) = b^T x = \sum_i b_i x_i
$$&lt;p&gt;梯度为：&lt;/p&gt;
$$
\nabla_x(b^T x) = b
$$&lt;p&gt;因此：&lt;/p&gt;
$$
\nabla f(x) = Ax + b
$$&lt;h3 id="b"&gt;b.
&lt;/h3&gt;&lt;p&gt;令 \(z = h(x)\)，则：&lt;/p&gt;
$$
f(x) = g(z) = g(h(x))
$$&lt;p&gt;对每个分量有：&lt;/p&gt;
$$
\frac{\partial f}{\partial x_i}
= g'(h(x)) \frac{\partial h(x)}{\partial x_i}
$$&lt;p&gt;因此：&lt;/p&gt;
&lt;div&gt;
$$
\nabla f(x)
=
\begin{pmatrix}
g'(h(x)) \frac{\partial h}{\partial x_1} \\
g'(h(x)) \frac{\partial h}{\partial x_2} \\
\vdots \\
g'(h(x)) \frac{\partial h}{\partial x_n}
\end{pmatrix}
=
g'(h(x)) \nabla h(x)
$$
&lt;/div&gt;
&lt;h3 id="c"&gt;c.
&lt;/h3&gt;&lt;p&gt;由 a 得：&lt;/p&gt;
$$
(\nabla f(x))_i
= \sum_{j=1}^n a_{ij} x_j + b_i
$$&lt;p&gt;Hessian 的第 \(i, j\) 项为：&lt;/p&gt;
&lt;div&gt;
$$
(\nabla^2 f(x))_{ij}
=
\frac{\partial}{\partial x_j}
\left(
\sum_{k=1}^n a_{ik} x_k + b_i
\right)
$$
&lt;/div&gt;
&lt;p&gt;利用：&lt;/p&gt;
$$
\frac{\partial}{\partial x_j}(a_{ik} x_k)
= a_{ik}\delta_{kj}
$$&lt;p&gt;所以：&lt;/p&gt;
$$
(\nabla^2 f(x))_{ij}
= \sum_{k=1}^n a_{ik}\delta_{kj}
= a_{ij}
$$&lt;p&gt;因此：&lt;/p&gt;
$$
\nabla^2 f(x) = A
$$</description></item><item><title>CS229 Lecture 2</title><link>https://lunatide.tech/p/cs229-lecture-2/</link><pubDate>Wed, 03 Dec 2025 00:00:00 +0000</pubDate><guid>https://lunatide.tech/p/cs229-lecture-2/</guid><description>&lt;img src="https://lunatide.tech/p/cs229-lecture-2/pic1.jpg" alt="Featured image of post CS229 Lecture 2" /&gt;&lt;h2 id="线性回归"&gt;线性回归
&lt;/h2&gt;&lt;p&gt;有如下数据集&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lunatide.tech/p/cs229-lecture-2/p1.jpg"
width="826"
height="326"
srcset="https://lunatide.tech/p/cs229-lecture-2/p1_hu_6de0275242601ffb.jpg 480w, https://lunatide.tech/p/cs229-lecture-2/p1_hu_f210478988d82bd6.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
&gt;&lt;/p&gt;
&lt;p&gt;在上图中，输入特征$x$是$\mathbb{R}^2$范围取值的一个二维向量，$x_1^{(i)}$就是训练集中第$i$个房屋的面积，而$x_2^{(i)}$就是训练集中第$i$个房屋的我是数量，这只是举个例子，设计算法的时候你可以自己设计特征量&lt;/p&gt;
&lt;p&gt;然后我们可以把$y$假设为一个以$x$为变量的线性函数
&lt;/p&gt;
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2
$$&lt;p&gt;
这里的$\theta_i$是参数，也叫权重，是从$X$到$Y$的线性函数映射的空间参数，在不引起混淆的情况下可以把$h_\theta(x)$中的$\theta$省略，另外，为了简化我们设$x_0=1$，简化后就有
&lt;/p&gt;
$$
h(x)=\sum_{i=0}^{n} \theta x_i=\theta^T x
$$&lt;p&gt;
等式最右边的$\theta$和$x$都是向量，$x$是输入变量的个数（就是特征量个数）&lt;/p&gt;
&lt;p&gt;现在，给定了一个&lt;strong&gt;训练集&lt;/strong&gt;，我们该如何挑选参数$\theta$，一个看上去比较合理的方法是让$h(x)$尽量逼近$y$,若是要用公式的形式来表示，就要定义一个函数，由此来衡量对于每个不同的$\theta$值，$h(x^{(i)})$与对应的$y^{(i)}$的距离，用如下的方式定义了一个&lt;strong&gt;成本函数&lt;/strong&gt;
&lt;/p&gt;
$$
J(\theta)=\frac{1}{2}\sum_{i=1}^{n}{(h_\theta(x^{(i)})-y^{(i)})^2}
$$&lt;p&gt;
你会发现这个函数和常规最小二乘法拟合模型中的最小二乘法成本函数非常相似&lt;/p&gt;
&lt;h3 id="最小均方算法lms"&gt;最小均方算法(LMS)
&lt;/h3&gt;&lt;p&gt;我们要让$J(\theta)$最小，我们考虑用&lt;strong&gt;梯度下降法&lt;/strong&gt;，这个方法就是从某一个$\theta$的初始值开始，然后逐渐重复更新
&lt;/p&gt;
$$
\theta_j:=\theta_j-\alpha \frac{\partial}{\partial \theta_j} J{(\theta)}
$$&lt;p&gt;
在这个式子中，$\alpha$是&lt;strong&gt;学习率&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;要实现这个算法，我们要知道右边的导数项是什么，让我们来计算一下
&lt;/p&gt;
$$
\begin{align*}
\frac{\partial}{\partial \theta_j} J(\theta)
&amp;= \frac{\partial}{\partial \theta_j} \frac{1}{2}(h_\theta(x) - y)^2 \\[6pt]
&amp;= (h_\theta(x) - y)\, \frac{\partial}{\partial \theta_j}(h_\theta(x) - y) \\[6pt]
&amp;= (h_\theta(x) - y)\, \frac{\partial}{\partial \theta_j}\left(\sum_{i=0}^n \theta_i x_i - y\right) \\[6pt]
&amp;= (h_\theta(x) - y)\, x_j
\end{align*}
$$&lt;p&gt;对单个训练样本，更新规则如下：&lt;/p&gt;
$$
\theta_j := \theta_j + \alpha \left( y^{(i)} - h_\theta(x^{(i)}) \right) x_j^{(i)}
$$&lt;p&gt;
这个规则也称为&lt;strong&gt;LMS&lt;/strong&gt;更新规则，也称为&lt;strong&gt;Widrow-Hoff&lt;/strong&gt;学习规则，具体的算法如下&lt;/p&gt;
&lt;p&gt;重复直到收敛{&lt;/p&gt;
&lt;p&gt;对每个$j$:
&lt;/p&gt;
$$
\theta_j:=\theta_j+\alpha\sum_{i=1}^m{(y^{(i)}-h_\theta(x^{(i)}))}
$$&lt;p&gt;
}&lt;/p&gt;
&lt;p&gt;这个方法叫做&lt;strong&gt;批量梯度下降法(batch gradient descent)&lt;/strong&gt;，此外还有一种方法&lt;/p&gt;
&lt;p&gt;Loop{&lt;/p&gt;
&lt;p&gt;​ for i=1 to m{&lt;/p&gt;
&lt;p&gt;​ 对每个$j$:
&lt;/p&gt;
$$
\theta_j := \theta_j + \alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}
$$&lt;p&gt;​ }&lt;/p&gt;
&lt;p&gt;}&lt;/p&gt;
&lt;p&gt;这个算法叫做&lt;strong&gt;随机梯度下降法(SGD)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在批量梯度下降算法中，我们要扫描整个训练集，才会更新一次，当数据集的量非常庞大的时候，会有很大计算量，而随机梯度下降算法在遇到训练样本的时候仅根据该单个训练样本的误差梯度更新参数，所以随机梯度下降往往比批量梯下降更快接近最小值。&lt;/p&gt;
&lt;p&gt;注意，它可能不会收敛到最小值，$\theta$会在$J(\theta)$的最小值附近震荡&lt;/p&gt;
&lt;h3 id="正规方程"&gt;正规方程
&lt;/h3&gt;&lt;p&gt;这是第二种方法，这种方法中我们通过求导让导数等于0的方式找到取得最小值的地方，给定一个训练集，把设计矩阵$X$设置为一个$x*n$的矩阵(实际上是$m * (n + 1)$，如果包含截距项)，该矩阵的每行是个训练样本
&lt;/p&gt;
$$
X =
\begin{bmatrix}
-(x^{(1)})^{T}- \\
-(x^{(2)})^T- \\
\vdots \\
-(x^{(m)})^T-
\end{bmatrix}
$$&lt;p&gt;另外，令 $\vec{y}$ 为包含训练集中所有目标值的 $m$ 维向量：&lt;/p&gt;
$$
\vec{y} =
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix}
$$&lt;p&gt;由于 $h_\theta(x^{(i)}) = (x^{(i)})^T \theta$，我们可以很容易地验证：&lt;/p&gt;
&lt;div&gt;
$$
X\theta - \vec{y} =
\begin{pmatrix}
(x^{(1)})^T\theta \\\
\vdots \\\
(x^{(m)})^T\theta
\end{pmatrix}
-
\begin{pmatrix}
y^{(1)} \\\
\vdots \\\
y^{(m)}
\end{pmatrix}
=
\begin{pmatrix}
(x^{(1)})^T\theta - y^{(1)} \\\
\vdots \\\
(x^{(m)})^T\theta - y^{(m)}
\end{pmatrix}
$$
&lt;/div&gt;
对于向量 $z$，则有 $z^T z = z^2$，因此利用这个性质，可以推导出：
$$
\begin{align*}
\frac{1}{2}(X\theta - \vec{y})^T (X\theta - \vec{y})
&amp;= \frac{1}{2} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
= J(\theta)
\end{align*}
$$&lt;p&gt;
关于$\theta$求梯度我们就可以得到：
&lt;/p&gt;
$$
\begin{align*}
\nabla_\theta J(\theta)
&amp;= \nabla_\theta \frac{1}{2} (X\theta - \vec{y})^T (X\theta - \vec{y}) \\[6pt]
&amp;= \frac{1}{2} \nabla_\theta (\theta^T X^T X \theta - \theta^T X^T \vec{y} - \vec{y}^T X \theta + \vec{y}^T \vec{y}) \\[6pt]
&amp;= \frac{1}{2} \nabla_\theta (\theta^T X^T X \theta - 2\theta^T X^T \vec{y}) \\[6pt]
&amp;= \frac{1}{2} (2 X^T X \theta - 2 X^T \vec{y}) \\[6pt]
&amp;= X^T X \theta - X^T \vec{y}
\end{align*}
$$&lt;p&gt;
第四个等号利用了&lt;/p&gt;
$$
\nabla_\theta (\theta^T A \theta) = (A + A^T)\theta
$$$$
\nabla_\theta (\theta^T x) = x
$$&lt;p&gt;令梯度为 0 可得 &lt;strong&gt;正规方程&lt;/strong&gt;：&lt;/p&gt;
$$
X^T X \theta = X^T \vec{y}
$$&lt;p&gt;因此，通过等式以解析形式给出使$J(\theta)$最小化的 $\theta$ 的值：&lt;/p&gt;
$$
\theta = (X^T X)^{-1} X^T \vec{y}
$$</description></item></channel></rss>