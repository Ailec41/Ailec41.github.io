<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="cs229 note"><title>CS229 Lecture 3</title><link rel=canonical href=https://lunatide.tech/p/cs229-lecture-3/><link rel=stylesheet href=https://lunatide.tech/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="CS229 Lecture 3"><meta property='og:description' content="cs229 note"><meta property='og:url' content='https://lunatide.tech/p/cs229-lecture-3/'><meta property='og:site_name' content="LunaTide's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2025-12-10T00:00:00+00:00'><meta property='article:modified_time' content='2025-12-10T00:00:00+00:00'><meta property='og:image' content='https://lunatide.tech/p/cs229-lecture-3/pic1.jpg'><meta name=twitter:title content="CS229 Lecture 3"><meta name=twitter:description content="cs229 note"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://lunatide.tech/p/cs229-lecture-3/pic1.jpg'><link rel="shortcut icon" href=https://lunatide.tech/./favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column compact"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=https://lunatide.tech/><img src=https://lunatide.tech/img/avatar_hu_8b96c345a57a41a4.jpeg width=300 height=297 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😓</span></figure><div class=site-meta><h1 class=site-name><a href=https://lunatide.tech/>LunaTide's Blog</a></h1><h2 class=site-description>Welcome to my Blog</h2></div></header><ol class=menu-social><li><a href=http://codeforces.com target=_blank title=codeforces rel=me><svg viewBox="0 0 24 24" id="code-forces"><path fill="#F44336" d="M24 19.5V12a1.5 1.5.0 00-1.5-1.5h-3A1.5 1.5.0 0018 12v7.5a1.5 1.5.0 001.5 1.5h3a1.5 1.5.0 001.5-1.5z"/><path fill="#2196F3" d="M13.5 21a1.5 1.5.0 001.5-1.5v-15A1.5 1.5.0 0013.5 3h-3C9.673 3 9 3.672 9 4.5v15c0 .828.673 1.5 1.5 1.5h3z"/><path fill="#FFC107" d="M0 19.5c0 .828.673 1.5 1.5 1.5h3A1.5 1.5.0 006 19.5V9A1.5 1.5.0 004.5 7.5h-3C.673 7.5.0 8.172.0 9v10.5z"/></svg></a></li><li><a href=https://www.zhihu.com target=_blank title=Zhihu rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-zhihu"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 6h6v12h-2l-2 2-1-2h-1z"/><path d="M4 12h6.5"/><path d="M10.5 6h-5"/><path d="M6 4c-.5 2.5-1.5 3.5-2.5 4.5"/><path d="M8 6v7c0 4.5-2 5.5-4 7"/><path d="M11 18l-3-5"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=https://lunatide.tech/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=https://lunatide.tech/%E5%85%B3%E4%BA%8E/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li><a href=https://lunatide.tech/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=https://lunatide.tech/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=https://lunatide.tech/%E5%8F%8B%E9%93%BE/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>友链</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=https://lunatide.tech/p/cs229-lecture-3/><img src=https://lunatide.tech/p/cs229-lecture-3/pic1_hu_eecca5964b47f3be.jpg srcset="https://lunatide.tech/p/cs229-lecture-3/pic1_hu_eecca5964b47f3be.jpg 800w, https://lunatide.tech/p/cs229-lecture-3/pic1_hu_f5ed2b2718725bf1.jpg 1600w" width=800 height=419 loading=lazy alt="Featured image of post CS229 Lecture 3"></a></div><div class=article-details><header class=article-category><a href=https://lunatide.tech/categories/deep-learning/>Deep Learning
</a><a href=https://lunatide.tech/categories/cs229/>CS229</a></header><div class=article-title-wrapper><h2 class=article-title><a href=https://lunatide.tech/p/cs229-lecture-3/>CS229 Lecture 3</a></h2><h3 class=article-subtitle>cs229 note</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2025-12-10</time></div></footer></div></header><section class=article-content><h2 id=最小二乘法的概率解释>最小二乘法的概率解释</h2><p>在面对回归问题的时候，我们会思考，为什么选择线性回归，为什么选择最小二乘法成本函数 <strong>J</strong> ？在本节里会给出一系列的概率基本假设，基于这些假设，可以推出最小二乘法是一种非常自然的算法</p><p>首先假设目标变量和输入值存在下面这种等量关系</p>$$
y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}
$$<p>上式中的 $\epsilon^{(i)}$ 是误差项，用于存放由于建模所忽略的变量导致的效果或者随机的噪音信息。进一步假设$\epsilon^{(i)}$是独立同分布的（IID），服从高斯分布，其平均值为0，方差为$\sigma^2$，这样就可以把这个假设写成"$\epsilon^{(i)} \sim N(0,\sigma^2)$"，然后$\epsilon^{(i)}$的密度函数就是：</p>$$
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(\epsilon^{(i)})^2}{2\sigma^2} \right)
$$<p>这意味着存在下面的等量关系：</p>$$
p(y^{(i)} \mid x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right)
$$<p>这里的记号 &ldquo;$p(y^{(i)} \mid x^{(i)}; \theta)$&ldquo;表示的是这是一个对于给定$x^{(i)}$的$y^{(i)}$的分布，用$\theta$进行了参数化，这里不能用&rdquo;$p(y^{(i)} \mid x^{(i)}, \theta)$&ldquo;来当作条件，因为$\theta$并不是一个随机变量，也可以将$y^{(i)}$的分布写成$y^{(i)} \mid x^{(i)};\theta \sim N(\theta^Tx^{(i)},\sigma^2)$</p><p>给定一个 $X$ 为设计矩阵，包含了全部$x^{(i)}$，然后再给定$\theta$，那么$y^{(i)}$的分布是什么？数据的概率由$p(\vec{y} \mid X;\theta)$的形式给出。在$\theta$取某个固定值的情况下，这个等式通常可以看作一个$\vec{y}$的函数。当我们把它当作 $\theta$ 的函数时，就称它为似然函数</p>$$
L(\theta)=L(\theta;X,\vec{y})= p(\vec{y} \mid X;\theta)
$$<p>结合之前对 $\epsilon^{(i)}$ 的独立性假设(这里对$y^{(i)}$ 以及给定的$x^{(i)}$也都做同样假设)，就可以把上面这个等式改写成下面的形式</p>$$
\begin{aligned}
L(\theta) &= \prod_{i=1}^{m} p(y^{(i)} \mid x^{(i)}; \theta) \\
&= \prod_{i=1}^{m} \left[ \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right) \right]
\end{aligned}
$$<p>现在，给定了 $y^{(i)}$ 和 $x^{(i)}$ 之间关系的概率模型了，用什么方法来选择咱们对参数 $\theta$ 的最佳猜测呢，最大似然法告诉我们要选择能让数据的似然函数尽可能大的 $\theta$ 。也就是说，咱们要找的 $\theta$ 能够让函数 $L(\theta)$ 取到最大值</p><p>为了运算方便，实际中我们选择最大化对数似然函数$l(\theta)$:</p>$$
\begin{aligned}
\ell(\theta) &= \log L(\theta) \\
&= \log \prod_{i=1}^{m} \left[ \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right) \right] \\
&= \sum_{i=1}^{m} \log \left[ \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right) \right] \\
&= m \log \frac{1}{\sqrt{2\pi}\sigma}
\;-\; \frac{1}{2\sigma^2} \sum_{i=1}^{m} (y^{(i)} - \theta^T x^{(i)})^2
\end{aligned}
$$<p>因此，对$l(\theta)$的最大值也就意味着下面这个子式取到最小值</p>$$
\frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - \theta^T x^{(i)})^2
$$<p>上式即为$J(\theta)$，我们最初的最小二乘成本函数</p><p>总结：在对数据进行概率假设的基础上，最小二乘回归得到的 $\theta$ 和最大似然法估计的 $\theta$ 是一致的。所以这是一系列的假设，其前提是认为最小二乘回归能够被判定为一种非常自然的方法，这种方法正好就进行了最大似然估计</p><p>还要注意，在刚才的讨论中，我们最终对 $\theta$ 的选择并不依赖 $\sigma^2$ ，而且也确实在不知道 $\sigma^2$ 的情况下就找到了结果。</p><h2 id=局部加权线性回归>局部加权线性回归</h2><p>假如问题从 $x \in R$来预测 y。下面第一幅图显示了使用 $y=\theta_0+\theta_1x$来对一个数据集来进行拟合。我们明显能看出来这个数据的趋势不是一条严格的直线，所以用直线进行的拟合就不是好的方法。</p><p><img src=https://lunatide.tech/p/cs229-lecture-3/p1.jpg width=1494 height=430 srcset="https://lunatide.tech/p/cs229-lecture-3/p1_hu_b6fc653ece4c3ee8.jpg 480w, https://lunatide.tech/p/cs229-lecture-3/p1_hu_493bdefc0d681192.jpg 1024w" loading=lazy class=gallery-image data-flex-grow=347 data-flex-basis=833px></p><p>那要是我们添加一个二次项，用 $y=\theta_0 + \theta_1x + \theta_2x^2$来拟合（上面中间的图），明显如果我们对特征补充得越多，效果就越好。不过增加太多的特征也会造成危险，看第三张图就是拟合5项多项式$y=\sum_{j=0}^{5}\theta_jx^j$的结果，可以看到，虽然拟合曲线完美地通过了所有当前数据集中的数据，但我们明显不能认为这个曲线是一个合适的预测工具，比如针对不同的居住面积 $x$ 来预测房屋价格 $y$ ，左边的图是一个<strong>欠拟合</strong>的例子，明显看到漏掉了数据集中的结构信息，而最右边的图是<strong>过拟合</strong>的例子</p><p>因此，如上面例子所示，特征的选择对于确保学习算法的良好性能很重要，在本节，我们会简单地谈谈局部加权线性回归算法，这里假设有足够的训练数据，使得特征选择不那么重要</p><p>在原始版本的线性回归算法中，要对一个查询点 $x$ 进行预测，比如要衡量 $h(x)$，要经过下面的步骤：</p><ul><li>使用参数 $\theta$ 进行拟合，让数据集中的值与拟合算出的值差值平方 $(y^{(i)} - \theta^Tx^{(i)})^2$最小（最小二乘法的思想）</li><li>输出 $\theta^Tx$</li></ul><p>相应地，在 LWR 局部加权线性回归的方法中，步骤如下：</p><ul><li>使用参数 $\theta$ 进行拟合，让加权距离$w^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$最小</li><li>输出$\theta^Tx$</li></ul><p>上面式子中的 $w^{(i)}$是非负的权值，直观地，如果$w^{(i)}$对于特定的 $i$ 很大，那么在选择 $\theta$ 时，我们将努力使 $(y^{(i)}-\theta^Tx^{(i)})$变小。如果$w^{(i)}$很小，则拟合中几乎忽略了 $(y^{(i)}-\theta^Tx^{(i)})^2$ 误差项</p><p>对于权值的选择可以使用下面这个比较标准的公式：</p>$$
w^{(i)}=\exp(- \frac{(x^{(i)}-x)^2}{2\tau^2} )
$$<p>参数$\tau$控制训练样本的权重随着其$x^{(i)}$距查询点$x$的距离而下降的速度，$\tau$称为<strong>带宽参数</strong></p><p>局部加权线性回归是我们看到的<strong>非参数</strong>算法的第一个例子。我们之前看到的(未加权)线性回归算法被称为<strong>参数</strong>算法，因为其具有固定的，有限数量的参数($\theta$)，这些参数由数据拟合。一旦我们拟合了$\theta_i$并将它们存储起来，我们就不再需要保留训练数据来做出对未来的预测，相反，如果用局部加权线性回归算法，我们就必须一直保留着整个训练集，这里的"非参数"粗略的指为了呈现出假设h遂着数据集的规模大增长而线性增长，我们需要用一定顺序保存一些数据的规模</p><h2 id=分类与逻辑回归>分类与逻辑回归</h2><p>分类问题其实和回归问题很像，只不过我们现在要来预测的$y$的值只局限于少数的若干个离散值。首先关注的是二值化分类问题，也就是说咋们要判断的 $y$ 只有两个取值，0或者1（这里说到的大多数内容也将推广到多类情况）。例如，如果我们正在尝试为电子邮件构建垃圾邮件分类器，则 $x^{(i)}$ 可能是电子邮件的某些特征，如果是垃圾邮件，则$y$为1，否则为0。0也称为<strong>负类</strong>，1表示<strong>正类</strong>，它们有时也用符号&rdquo;-&ldquo;和&rdquo;+&ldquo;表示，给定$x^{(i)}$，相应的$y^{(i)}$也称为训练样本的<strong>标签</strong></p><h3 id=logistic回归>Logistic回归</h3><p>我们可以忽略$y$是离散值的事实来处理分类问题，并使用我们的旧线性回归算法来尝试预测给定的$x$的$y$。但是，很容易构造此方法的效果非常差的例子。直觉上，当我们知道$y \in {0,1}$，所以$h_0(x)$的值如果大于1或者小于0就没有意义了，就是说$y$的值必然应当是0和1这两个值中的一个</p><p>所以咱们就改变一下假设函数$h_0(x)$的形式，来解决这个问题。比如咱们可以选择下面这个函数：</p>$$
h_0(x)=g(\theta^Tx)= \frac{1}{1+e^{-\theta^Tx}}
$$<p>其中有:</p>$$
g(z) = \frac{1}{1+e^{-z}}
$$<p>这个函数就是我们所熟悉的sigmoid函数，也叫做logistic函数，下面是它的图像</p><p><img src=https://lunatide.tech/p/cs229-lecture-3/p2.jpg width=976 height=694 srcset="https://lunatide.tech/p/cs229-lecture-3/p2_hu_2367dfbb24b80f4.jpg 480w, https://lunatide.tech/p/cs229-lecture-3/p2_hu_97bac66fadd7ff1f.jpg 1024w" loading=lazy class=gallery-image data-flex-grow=140 data-flex-basis=337px></p><p>注意到，当 $z \to \infty$ 时 $g(z)$ 趋近于 1，而当 $z \to -\infty$ 时 $g(z)$ 趋近于 0。 此外，$g(z)$ 和 $h(x)$ 的值总是在 0 和 1 之间波动。 我们保持 $x_0 = 1$ 的约定，所以$\theta^T x = \theta_0 + \sum_{j=1}^{n} \theta_j x_j$</p><p>现在咱们就把 g 作为选定的函数了。当然其他的从0到1之间光滑递增的函数也可以使用，不过后面我们会了解到选择g的一些原因，对这个逻辑函数的选择是很自然的。在继续深入之前，在继续深入之前，下面通过导数讲解下这个函数的一些性质</p>$$
\begin{aligned}
g'(z) &= \frac{d}{dz} \frac{1}{1 + e^{-z}} \\
&= \frac{1}{(1 + e^{-z})^{2}} \, (e^{-z}) \\
&= \frac{1}{(1 + e^{-z})} \left( 1 - \frac{1}{1 + e^{-z}} \right) \\
&= g(z)(1 - g(z)).
\end{aligned}
$$<p>给定了逻辑回归模型了，咱们怎么去拟合一个合适的 $\theta$ 呢？我们之前已经看到了在一系列前提下，最小二乘法回归可以通过最大似然估计来推出，那么接下来就给我们这个分类模型做一系列的统计学假设，然后用最大似然法拟合参数</p><p>首先假设：</p>$$
P(y = 1 \mid x;\theta) = h_\theta(x)
$$$$
P(y = 0 \mid x;\theta) = 1 - h_\theta(x)
$$<p>更简洁的写法是：</p>$$
p(y \mid x;\theta) = (h_\theta(x))^{y}\,(1 - h_\theta(x))^{1 - y}
$$<p>假设m个训练样本都是各自独立生成的，那么就可以按如下的方式来写参数的似然函数：</p>$$
\begin{aligned}
L(\theta) &= p(\vec{y} \mid X; \theta) \\
&= \prod_{i=1}^{m} p(y^{(i)} \mid x^{(i)}; \theta) \\
&= \prod_{i=1}^{m} \left( h_\theta(x^{(i)})^{\,y^{(i)}} \, (1 - h_\theta(x^{(i)}))^{\,1 - y^{(i)}} \right)
\end{aligned}
$$<p>然后和之前一样，取个对数就很容易计算最大值</p>$$
\begin{aligned}
\ell(\theta) &= \log L(\theta) \\
&= \sum_{i=1}^{m}\Big( y^{(i)} \log h(x^{(i)}) + \big(1 - y^{(i)}\big)\log\big(1 - h(x^{(i)})\big) \Big)
\end{aligned}
$$<p>怎么让似然函数最大？就跟之前在线性回归的时候用了求导数的方法类似，咱们这次就是用<strong>梯度上升法</strong>。还是写成向量的形式，然后更新，就是$\theta := \theta + \alpha \nabla_{\theta}\ell(\theta)$。（因为找最大值，所以是加号），还是先从只有一组训练样本(x,y)开始，然后求导数来退出随机梯度上升规则：</p>$$
\begin{aligned}
\frac{\partial}{\partial \theta_j}\ell(\theta)
&= \left( y \frac{1}{g(\theta^T x)} - (1-y)\frac{1}{1-g(\theta^T x)} \right)
\frac{\partial}{\partial \theta_j} g(\theta^T x) \\
&= \left( y \frac{1}{g(\theta^T x)} - (1-y)\frac{1}{1-g(\theta^T x)} \right)
g(\theta^T x)\bigl(1-g(\theta^T x)\bigr)\,
\frac{\partial}{\partial \theta_j}\theta^T x \\
&= \Bigl( y\bigl(1-g(\theta^T x)\bigr) - (1-y)g(\theta^T x) \Bigr)\,x_j \\
&= \bigl( y - h_\theta(x) \bigr)\,x_j
\end{aligned}
$$<p>上面的式子里，我们用到了对函数求导的定理$g&rsquo;(z)=g(z)(1-g(z))$。然后就用到了随机梯度上升规则：</p>$$
\theta_j := \theta_j + \alpha\bigl(y^{(i)}-h_\theta(x^{(i)})\bigr)x_j^{(i)}
$$<p>如果我们将其和 <strong>LMS</strong> 更新规则相对比，就能发现看上去挺相似的；<strong>不过这并不是同一个算法</strong>，因为这里的$h_0(x^{(i)})$现在定义成了一个$\theta^Tx^{(i)}$的非线性函数尽管如此，我们面对不同的学习问题使用了不同的算法，却得到了看上去一样的更新规则，这是巧合吗，我们学到GLM广义线性模型的时候就会得到答案了。</p><h2 id=题外话感知器学习算法>题外话：感知器学习算法</h2><p>现在简单聊一个算法，它的历史很有趣，并且之后讲学习理论的时候还要讲到它。设想一下，对逻辑回归方法修改一下，“强迫” 它输出的值要么是0要么是1。要实现这个目的，很自然就应该把函数 $g$ 的定义修改一下，改成一个阙值函数</p>$$
g(z)=
\begin{cases}
1, & z \ge 0 \\
0, & z < 0
\end{cases}
$$<p>若是我们还像之前一样令 $h_\theta(x)=g(\theta^Tx)$，但用刚刚上面的阙值函数作为 $g$ 的定义，然后如果我们用了下面的更新规则：</p>$$
\theta_j := \theta_j + \alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}
$$<p>这样我们就得到了感知器学习算法</p></section><footer class=article-footer></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=https://lunatide.tech/p/cs229%E4%BD%9C%E4%B8%9A0/><div class=article-image><img src=https://lunatide.tech/p/cs229%E4%BD%9C%E4%B8%9A0/pic1.e3fe3f7deb19da40529efe3fac8bb5cd_hu_31804b513141a106.jpg width=250 height=150 loading=lazy alt="Featured image of post CS229作业0" data-hash="md5-4/4/fesZ2kBSnv4/rIu1zQ=="></div><div class=article-details><h2 class=article-title>CS229作业0</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs229-lecture-2/><div class=article-image><img src=https://lunatide.tech/p/cs229-lecture-2/pic1.e3fe3f7deb19da40529efe3fac8bb5cd_hu_31804b513141a106.jpg width=250 height=150 loading=lazy alt="Featured image of post CS229 Lecture 2" data-hash="md5-4/4/fesZ2kBSnv4/rIu1zQ=="></div><div class=article-details><h2 class=article-title>CS229 Lecture 2</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs231-%E7%AC%AC%E5%9B%9B%E8%AE%B2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/><div class=article-image><img src=https://lunatide.tech/p/cs231-%E7%AC%AC%E5%9B%9B%E8%AE%B2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/pic1.543c3ec3703e3e550061d7dc2576aef5_hu_247d997126bf848d.jpg width=250 height=150 loading=lazy alt="Featured image of post CS231 第四讲 神经网络与反向传播" data-hash="md5-VDw+w3A+PlUAYdfcJXau9Q=="></div><div class=article-details><h2 class=article-title>CS231 第四讲 神经网络与反向传播</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs231-%E7%AC%AC%E4%B8%89%E8%AE%B2-%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E4%BC%98%E5%8C%96/><div class=article-image><img src=https://lunatide.tech/p/cs231-%E7%AC%AC%E4%B8%89%E8%AE%B2-%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E4%BC%98%E5%8C%96/pic1.543c3ec3703e3e550061d7dc2576aef5_hu_247d997126bf848d.jpg width=250 height=150 loading=lazy alt="Featured image of post CS231 第三讲 正则化与优化" data-hash="md5-VDw+w3A+PlUAYdfcJXau9Q=="></div><div class=article-details><h2 class=article-title>CS231 第三讲 正则化与优化</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs231-%E7%AC%AC%E4%BA%8C%E8%AE%B2-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/><div class=article-image><img src=https://lunatide.tech/p/cs231-%E7%AC%AC%E4%BA%8C%E8%AE%B2-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/pic1.543c3ec3703e3e550061d7dc2576aef5_hu_247d997126bf848d.jpg width=250 height=150 loading=lazy alt="Featured image of post CS231 第二讲 图像分类" data-hash="md5-VDw+w3A+PlUAYdfcJXau9Q=="></div><div class=article-details><h2 class=article-title>CS231 第二讲 图像分类</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 LunaTide's Blog</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=https://lunatide.tech/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>