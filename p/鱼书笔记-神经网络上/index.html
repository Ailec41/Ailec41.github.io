<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="deep learning note"><title>鱼书笔记-神经网络(上)</title><link rel=canonical href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/><link rel=stylesheet href=https://lunatide.tech/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="鱼书笔记-神经网络(上)"><meta property='og:description' content="deep learning note"><meta property='og:url' content='https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/'><meta property='og:site_name' content="LunaTide's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2025-11-10T00:00:00+00:00'><meta property='article:modified_time' content='2025-11-10T00:00:00+00:00'><meta property='og:image' content='https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic3.jpg'><meta name=twitter:title content="鱼书笔记-神经网络(上)"><meta name=twitter:description content="deep learning note"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic3.jpg'><link rel="shortcut icon" href=https://lunatide.tech/./favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=https://lunatide.tech/><img src=https://lunatide.tech/img/avatar_hu_8b96c345a57a41a4.jpeg width=300 height=297 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😓</span></figure><div class=site-meta><h1 class=site-name><a href=https://lunatide.tech/>LunaTide's Blog</a></h1><h2 class=site-description>Welcome to my Blog</h2></div></header><ol class=menu-social><li><a href=http://codeforces.com target=_blank title=codeforces rel=me><svg viewBox="0 0 24 24" id="code-forces"><path fill="#F44336" d="M24 19.5V12a1.5 1.5.0 00-1.5-1.5h-3A1.5 1.5.0 0018 12v7.5a1.5 1.5.0 001.5 1.5h3a1.5 1.5.0 001.5-1.5z"/><path fill="#2196F3" d="M13.5 21a1.5 1.5.0 001.5-1.5v-15A1.5 1.5.0 0013.5 3h-3C9.673 3 9 3.672 9 4.5v15c0 .828.673 1.5 1.5 1.5h3z"/><path fill="#FFC107" d="M0 19.5c0 .828.673 1.5 1.5 1.5h3A1.5 1.5.0 006 19.5V9A1.5 1.5.0 004.5 7.5h-3C.673 7.5.0 8.172.0 9v10.5z"/></svg></a></li><li><a href=https://www.zhihu.com target=_blank title=Zhihu rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-zhihu"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 6h6v12h-2l-2 2-1-2h-1z"/><path d="M4 12h6.5"/><path d="M10.5 6h-5"/><path d="M6 4c-.5 2.5-1.5 3.5-2.5 4.5"/><path d="M8 6v7c0 4.5-2 5.5-4 7"/><path d="M11 18l-3-5"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=https://lunatide.tech/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=https://lunatide.tech/%E5%85%B3%E4%BA%8E/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li><a href=https://lunatide.tech/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=https://lunatide.tech/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=https://lunatide.tech/%E5%8F%8B%E9%93%BE/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>友链</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#从感知机到神经网络>从感知机到神经网络</a><ol><li><a href=#感知机回顾>感知机回顾</a></li><li><a href=#激活函数引入>激活函数引入</a></li></ol></li><li><a href=#激活函数>激活函数</a><ol><li><a href=#sigmoid函数>sigmoid函数</a></li><li><a href=#阶跃函数的实现>阶跃函数的实现</a></li><li><a href=#阶跃函数的图形>阶跃函数的图形</a></li><li><a href=#sigmoid函数的实现>sigmoid函数的实现</a></li><li><a href=#sigmoid函数和阶跃函数的比较>sigmoid函数和阶跃函数的比较</a></li><li><a href=#非线性函数>非线性函数</a></li><li><a href=#relu函数>ReLU函数</a></li></ol></li><li><a href=#多维数组的运算>多维数组的运算</a><ol><li><a href=#多维数组>多维数组</a></li><li><a href=#矩阵乘法>矩阵乘法</a></li><li><a href=#神经网络的内积>神经网络的内积</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic3_hu_7244c689e98ede3c.jpg srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic3_hu_7244c689e98ede3c.jpg 800w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic3_hu_1ef3c590ecabf062.jpg 1600w" width=800 height=446 loading=lazy alt="Featured image of post 鱼书笔记-神经网络(上)"></a></div><div class=article-details><header class=article-category><a href=https://lunatide.tech/categories/deep-learning/>Deep Learning</a></header><div class=article-title-wrapper><h2 class=article-title><a href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/>鱼书笔记-神经网络(上)</a></h2><h3 class=article-subtitle>deep learning note</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2025-11-10</time></div></footer></div></header><section class=article-content><p><strong>以下内容皆基于鱼书《深度学习入门基于python的理论与实现》</strong></p><h2 id=从感知机到神经网络>从感知机到神经网络</h2><h3 id=感知机回顾>感知机回顾</h3><p>用图来表示神经网络，类比感知机，我们把左边的一列称为<strong>输入层</strong>，最右边的称之为<strong>输出层</strong>，中间的称为<strong>中间层</strong>(也称为隐藏层，因为神经元肉眼看不见)，我们知道当感知机接受$x_1,x_2$两个输入信号，输出$y$时，可以用如下的数学式来表示</p>$$
y =
\begin{cases}
0, & b + w_1 x_1 + w_2 x_2 \le 0 \\
1, & b + w_1 x_1 + w_2 x_2 > 0
\end{cases}
\tag{1}
$$<p>$b$是偏置，用于控制神经元被激活的容易程度，而$w_1,w_2$是表示各个信号的权重的参数，用于控制各个信号的重要性</p><p>我们现在可以通过调用一个函数来替代(1)中分case讨论的情况来简化(1)，改写成如下形式</p>$$
y = h(b + w_1x_1+ w_2x_2)\tag{2}
$$$$
h(x) =
\begin{cases}
0, & x \le 0 \\
1, & x > 0
\end{cases}\tag{3}
$$<h3 id=激活函数引入>激活函数引入</h3><p>刚才的h(x)把输入信号的总和转换成了输出信号，h(x)就被称为<strong>激活函数(activation function)</strong></p><p>现在进一步改写式(2)，写成如下形式</p>$$
a = b + w_1x_1 + w_2x_2\tag{4}
$$$$
y = h(a)\tag{5}
$$<p>首先，式(4)计算加权输入信号的和偏置的总和，然后用(5)的h函数转换为输出</p><h2 id=激活函数>激活函数</h2><h3 id=sigmoid函数>sigmoid函数</h3><p>神经网络中经常使用的一个激活函数就是<strong>sigmoid函数</strong></p>$$
h(x)=\frac{1}{1+e^{-x}} \quad (\text{sigmoid function})\tag{6}
$$<p>实际上，感知机和神经网络的主要区别就在于激活函数，其他方面基本都是一样的</p><h3 id=阶跃函数的实现>阶跃函数的实现</h3><p>阶跃函数如(3)所示，当输入超过0时，输出1，否则输出0，可以用如下代码简单实现</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>step_function</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>x</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>  <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span>
</span></span></code></pre></td></tr></table></div></div><p>这个代码中参数x只能接受实数。例如不允许<code>step_function(np.array([1.0,2.0]))</code>，所以我们把它修改为支持NumPy数组的实现</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>step_function</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>&gt;</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>y</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>int</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=阶跃函数的图形>阶跃函数的图形</h3><p>接下来我们就用图来表示上面定义的阶跃函数</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pylab</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>step_function</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>,</span><span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=o>-</span><span class=mf>5.0</span><span class=p>,</span><span class=mf>5.0</span><span class=p>,</span><span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>step_function</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylim</span><span class=p>(</span><span class=o>-</span><span class=mf>0.1</span><span class=p>,</span><span class=mf>1.1</span><span class=p>)</span> <span class=c1>#y轴范围</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/figure1.jpg width=640 height=480 srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/figure1_hu_4edc352cb7851f7c.jpg 480w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/figure1_hu_893ac30a708dd13a.jpg 1024w" loading=lazy class=gallery-image data-flex-grow=133 data-flex-basis=320px></p><h3 id=sigmoid函数的实现>sigmoid函数的实现</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>sigmoid</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>x</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>之所以sigmoid函数的实现支持NumPy数组，就是因为NumPy的广播功能，如果在标量和NumPy数组之间进行运算，标量会和NumPy数组的各个元素进行运算，<code>np.exp(-x)</code>会生成NumPy数组，所以<code>1/(1 + np.exp(-x))</code>的运算将会在NumPy数组的各个元素间进行</p><h3 id=sigmoid函数和阶跃函数的比较>sigmoid函数和阶跃函数的比较</h3><p><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic4.jpg width=1070 height=1034 srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic4_hu_15fd3252af9c0d75.jpg 480w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic4_hu_8cd92bb4638acb60.jpg 1024w" loading=lazy alt=阶跃函数与sigmoid函数 class=gallery-image data-flex-grow=103 data-flex-basis=248px></p><p>观察可以发现，首先区别就是平滑性，sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。因此我们可以知道，感知机的神经元之间流动的是0或1的二元信号，神经网络中流动的是连续的实数值信号。</p><p>然后说一下阶跃函数和sigmoid函数的共同性质，两者的结构均是“输入小时输出接近0；输入大时，输出靠近1”，以及不管输入是什么值，输出信号的值都在0和1中间</p><h3 id=非线性函数>非线性函数</h3><p>阶跃函数和sigmoid函数都是非线性函数</p><p>神经网络的激活函数必须使用非线性函数，因为如果使用线性函数，加深神经网络的层数就没有意义了（应该很好理解，很多线型函数复合仍然是线性的，就不具体说了）</p><h3 id=relu函数>ReLU函数</h3><p>最近比较常见的是ReLU函数</p><p>ReLU函数在输入大于0时，直接输出该值；在输入小于等于0的时候，输出0</p><p><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic5.jpg width=1088 height=716 srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic5_hu_643511c3d68d1186.jpg 480w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8A/pic5_hu_6dbfa2b46e66261.jpg 1024w" loading=lazy class=gallery-image data-flex-grow=151 data-flex-basis=364px></p><p>ReLU函数可以表示为以下数学式</p>$$
h(x) =\begin{cases}
x, & x > 0 \\
0, & x \le 0
\end{cases}
\tag{7}
$$<p>ReLU函数的实现也非常简单</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>relu</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=多维数组的运算>多维数组的运算</h2><h3 id=多维数组>多维数组</h3><p>首先假定有一个一维数组<code>A = np.array[1,2,3,4]</code>，数组的维数可以通过<code>np.ndim</code>得到。数组的形状可以通过实例变量shape获得，A由四个元素构成，是一维的，所以A.shape就是（4，），这个结果是个元组，这个一维数组为了保证和多维一样的格式，所以仍然被写成元组</p><h3 id=矩阵乘法>矩阵乘法</h3><p>不再赘述</p><h3 id=神经网络的内积>神经网络的内积</h3></section><footer class=article-footer></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E4%B8%8E%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%9A%84%E6%8A%80%E5%B7%A7/><div class=article-image><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E4%B8%8E%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%9A%84%E6%8A%80%E5%B7%A7/pic3.ea994d84be7267c39cdac1325390798d_hu_5f7ccc2673021dd8.jpg width=250 height=150 loading=lazy alt="Featured image of post 鱼书笔记-与学习相关的技巧" data-hash="md5-6plNhL5yZ8Oc2sEyU5B5jQ=="></div><div class=article-details><h2 class=article-title>鱼书笔记-与学习相关的技巧</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95/><div class=article-image><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95/pic3.ea994d84be7267c39cdac1325390798d_hu_5f7ccc2673021dd8.jpg width=250 height=150 loading=lazy alt="Featured image of post 鱼书笔记-误差反向传播法" data-hash="md5-6plNhL5yZ8Oc2sEyU5B5jQ=="></div><div class=article-details><h2 class=article-title>鱼书笔记-误差反向传播法</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/><div class=article-image><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/pic3.ea994d84be7267c39cdac1325390798d_hu_5f7ccc2673021dd8.jpg width=250 height=150 loading=lazy alt="Featured image of post 鱼书笔记-神经网络的学习" data-hash="md5-6plNhL5yZ8Oc2sEyU5B5jQ=="></div><div class=article-details><h2 class=article-title>鱼书笔记-神经网络的学习</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/><div class=article-image><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/pic3.ea994d84be7267c39cdac1325390798d_hu_5f7ccc2673021dd8.jpg width=250 height=150 loading=lazy alt="Featured image of post 鱼书笔记-神经网络(下)" data-hash="md5-6plNhL5yZ8Oc2sEyU5B5jQ=="></div><div class=article-details><h2 class=article-title>鱼书笔记-神经网络(下)</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 LunaTide's Blog</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=https://lunatide.tech/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>