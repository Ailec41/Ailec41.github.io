<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="deep learning note"><title>鱼书笔记-神经网络的学习</title><link rel=canonical href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/><link rel=stylesheet href=https://lunatide.tech/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="鱼书笔记-神经网络的学习"><meta property='og:description' content="deep learning note"><meta property='og:url' content='https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/'><meta property='og:site_name' content="LunaTide's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2025-11-17T00:00:00+00:00'><meta property='article:modified_time' content='2025-11-17T00:00:00+00:00'><meta property='og:image' content='https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/pic3.jpg'><meta name=twitter:title content="鱼书笔记-神经网络的学习"><meta name=twitter:description content="deep learning note"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/pic3.jpg'><link rel="shortcut icon" href=https://lunatide.tech/./favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column compact"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=https://lunatide.tech/><img src=https://lunatide.tech/img/avatar_hu_8b8c7ec71f46c47e.jpeg width=300 height=297 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😓</span></figure><div class=site-meta><h1 class=site-name><a href=https://lunatide.tech/>LunaTide's Blog</a></h1><h2 class=site-description>Welcome to my Blog</h2></div></header><ol class=menu-social><li><a href=http://codeforces.com target=_blank title=codeforces rel=me><svg viewBox="0 0 24 24" id="code-forces"><path fill="#F44336" d="M24 19.5V12a1.5 1.5.0 00-1.5-1.5h-3A1.5 1.5.0 0018 12v7.5a1.5 1.5.0 001.5 1.5h3a1.5 1.5.0 001.5-1.5z"/><path fill="#2196F3" d="M13.5 21a1.5 1.5.0 001.5-1.5v-15A1.5 1.5.0 0013.5 3h-3C9.673 3 9 3.672 9 4.5v15c0 .828.673 1.5 1.5 1.5h3z"/><path fill="#FFC107" d="M0 19.5c0 .828.673 1.5 1.5 1.5h3A1.5 1.5.0 006 19.5V9A1.5 1.5.0 004.5 7.5h-3C.673 7.5.0 8.172.0 9v10.5z"/></svg></a></li><li><a href=https://www.zhihu.com target=_blank title=Zhihu rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-zhihu"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 6h6v12h-2l-2 2-1-2h-1z"/><path d="M4 12h6.5"/><path d="M10.5 6h-5"/><path d="M6 4c-.5 2.5-1.5 3.5-2.5 4.5"/><path d="M8 6v7c0 4.5-2 5.5-4 7"/><path d="M11 18l-3-5"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=https://lunatide.tech/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=https://lunatide.tech/%E5%85%B3%E4%BA%8E/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li><a href=https://lunatide.tech/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=https://lunatide.tech/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=https://lunatide.tech/%E5%8F%8B%E9%93%BE/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>友链</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/pic3_hu_c76f475c7462b4a9.jpg srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/pic3_hu_c76f475c7462b4a9.jpg 800w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/pic3_hu_fb2d39cddbc14bec.jpg 1600w" width=800 height=446 loading=lazy alt="Featured image of post 鱼书笔记-神经网络的学习"></a></div><div class=article-details><header class=article-category><a href=https://lunatide.tech/categories/deep-learning/>Deep Learning</a></header><div class=article-title-wrapper><h2 class=article-title><a href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/>鱼书笔记-神经网络的学习</a></h2><h3 class=article-subtitle>deep learning note</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2025-11-17</time></div></footer></div></header><section class=article-content><h2 id=从数据中学习>从数据中学习</h2><p>神经网络的特征就是从数据中学习（由数据自动决定权重参数的值）</p><h3 id=数据驱动>数据驱动</h3><p>我们接着上一章最后手写数字识别的话题，思考一下会发现如果设计一个能自动识别5的算法还是挺困难的（至少我是这样认为的），所以我们应该考虑通过有效利用数据来解决这个问题，一种方案是从图像中提取特征量，再用机器学习技术学习这些特征量的模式</p><p>机器学习的方法中，由机器从收集到的数据中找到规律性。但是将图像转换为向量时使用的特征量仍是由人设计的，对于不同的问题，必须使用合适的特征量，才能得到好的结果</p><p>还有一种是神经网络（深度学习）的方法，该方法不存在人为介入，神经网络会直接学习图像本身</p><h3 id=训练数据和测试数据>训练数据和测试数据</h3><p>机器学习中把数据分成训练数据和测试数据两部分，首先用训练数据进行学习，寻找最优的参数，然后用测试数据评价训练得到的模型的实际能力，为了正确评价模型的泛化能力，就必须划分训练数据和测试数据，训练数据也被称作<strong>监督数据</strong></p><p>泛化能力是指处理未被观察过的数据的能力。机器学习的目标就是为了提高泛化能力</p><p>因此，仅仅用一个数据集去学习和评价参数，无法正确评价，只用某个数据集过度拟合的状态称为过拟合</p><h2 id=损失函数>损失函数</h2><p>神经网络的学习通过某个指标来表示现在的状态。然后以这个指标为基准，寻找最优权重参数。这个指标被称为<strong>损失函数</strong>。损失函数可以使用任意参数，但一般用均方误差和交叉熵误差等。</p><h3 id=均方误差>均方误差</h3><p>如下式</p>$$
E = \frac{1}{2} \sum_k (y_k - t_k) ^ 2 \tag{1}
$$<p>这里$y_k$是表示神经网络的输出，$t_k$是表示监督数据，$k$表示数据的维数，如式(1)所示，均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方，再求总和。python实现均方误差的实现方式如下所示</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>mean_squared_error</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=n>y</span> <span class=o>-</span> <span class=n>t</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=交叉熵误差>交叉熵误差</h3><p>交叉熵误差如下式所示</p>$$
E = - \sum_k (t_k \log{y_k}) \tag{2}
$$<p>$y_k$是神经网络的输出，$t_k$是正确解标签(采用one-hot表示)。交叉熵误差的值是由正确解标签所对应的输出结果决定的。</p><p>根据对数函数的性质我们可以知道，正确解标签对应的输出越大，式(2)的值就越靠近0；输出为1时，交叉熵的误差为0。如果正确解标签对应的输出较小，(2)的值就越大。</p><p>下面实现一下交叉熵误差</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>cross_entropy_error</span><span class=p>(</span><span class=n>y</span><span class=p>,</span><span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>delta</span> <span class=o>=</span> <span class=mi>1</span><span class=n>e</span> <span class=o>-</span> <span class=mi>7</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>t</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>y</span> <span class=o>+</span> <span class=n>delta</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>y和t在这里是NumPy数组，加上一个delta是为了防止-inf的发生</p><h3 id=mini-batch学习>mini-batch学习</h3><p>前面说的都是单个数据的损失函数。如果要求所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成下面的式(3)</p>$$
E = -\frac{1}{N} \sum_{n} \sum_{k} t_{nk}\,\log y_{nk} \tag{3}
$$<p>假设一共有N个数据，$t_{nk}$表示第n个数据的第k个元素的值</p><p>这个式子就是把单个数据的损失函数的式扩大到了N份数据，不过最后还要除以N进行正规化。</p><p>MNIST数据集的训练数据有60000个，用全部数据来计算损失函数的值所花费的时间太长，所以我们从中选取一部分。神经网络的学习也是从训练数据中选出一批数据（称为mini-batch)，然后对每个mini-batch进行学习。</p><h3 id=mini-batch版交叉熵误差的实现>mini-batch版交叉熵误差的实现</h3><p>对于mini-batch的交叉熵误差，只要改良一下之前实现对应单个数据的交叉熵误差就可以。这里实现一个可以同时处理单个数据和批量数据两种情况的函数</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>cross_entropy_error</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>y</span><span class=o>.</span><span class=n>ndim</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>t</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=n>t</span><span class=o>.</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>y</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=n>y</span><span class=o>.</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>  <span class=n>batch_size</span> <span class=o>=</span> <span class=n>y</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=p>,</span><span class=n>log</span><span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>batch_size</span><span class=p>),</span><span class=n>t</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=n>e</span> <span class=o>-</span> <span class=mi>7</span><span class=p>))</span> <span class=o>/</span> <span class=n>batch_size</span>
</span></span></code></pre></td></tr></table></div></div><p>这里，y是神经网络的输出，t是监督数据。y的维度为1时，即求单个数据的交叉熵误差时，需要改变数据的形状。并且，当输入为mini-batch时，要用batch的个数进行正规化，计算单个数据的平均交叉熵误差</p><p>此外，当监督数据时标签形式(非one-hot表示，而是像"2" &ldquo;7"这样的)交叉熵误差可以如下实现</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>cross_entropy_error</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>y</span><span class=o>.</span><span class=n>ndim</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>t</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=n>t</span><span class=o>.</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>y</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=n>y</span><span class=o>.</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>  <span class=n>batch_size</span> <span class=o>=</span> <span class=n>y</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>arrange</span><span class=p>(</span><span class=n>batch_size</span><span class=p>),</span><span class=n>t</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=n>e</span> <span class=o>-</span> <span class=mi>7</span><span class=p>))</span> <span class=o>/</span> <span class=n>batch_size</span>
</span></span></code></pre></td></tr></table></div></div><p>由于one-hot表示中t为0的元素的交叉熵误差也为0，因此针对这些元素的计算可以忽略。只要可以获得神经网络在正确解标签的输出，就可以计算交叉熵误差,t为one-hot表示时通过<code>t * np.log(y)</code>计算的地方t为标签形式时，可以用<code>np.log(y[np.arange(batch_size),t])</code>表示实现相同的处理</p><h3 id=为什么要设定损失函数>为什么要设定损失函数</h3><p>假设有一个神经网络，对其中一个权重参数的损失函数求导，如果这个导数的值为负，说明使该权重参数向正正方向改变，可以减小损失函数的值；反之亦然，以及当导数的值为0时候，无论权重参数往哪个方向，损失函数的值都不会改变。而如果用识别精度作为指标，则参数的导数在绝大多数地方都为0</p><h2 id=梯度法>梯度法</h2><p>梯度的方向不一定指向最小值，但是沿着梯度的方向能够最大限度地减小函数的值</p><p>梯度法是什么，就是让函数的取值沿着梯度的方向前进一段距离，在新的地方重新求梯度，然后再沿着梯度方向前进，像这样反复，逐渐减小函数值，然后我们用数学式来表示梯度法，如下式(4)</p>$$
x_0 = x_0 - \eta \frac{\partial f}{\partial x_0}\\
x_1 = x_1 - \eta \frac{\partial f}{\partial x_1}\tag{4}
$$<p>上式的$\eta$表示更新量，在神经网络的学习中，称为学习率，决定了在一次学习中，应该学习多少，以及在多大程度上更新参数</p><p>接下来用python实现下梯度下降法</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>gradient_descet</span><span class=p>(</span><span class=n>f</span><span class=p>,</span><span class=n>init_x</span><span class=p>,</span><span class=n>lr</span> <span class=o>=</span> <span class=mf>0.01</span><span class=p>,</span><span class=n>step_num</span> <span class=o>=</span> <span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>x</span> <span class=o>=</span> <span class=n>init_x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>step_num</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>grad</span> <span class=o>=</span> <span class=n>numerical_gradient</span><span class=p>(</span><span class=n>f</span><span class=p>,</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>-=</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>grad</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></td></tr></table></div></div><p>参数f是要进行最优化的函数，init_x是初始值，lr是学习率，step_num是梯度法的重复次数，numerical_gradient(f,x)会求函数的梯度</p><h3 id=神经网络的梯度>神经网络的梯度</h3><p>神经网络的学习也要求梯度，这里所说的梯度是指损失函数关于权重参数的梯度，例如一个形状2x3的权重$W$的神经网络，损失函数用L表示。此时，梯度可以用$\frac{\partial L}{\partial \mathbf{W}}$表示</p>$$
\mathbf{W} =
\begin{pmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23}
\end{pmatrix}\\
\frac{\partial L}{\partial \mathbf{W}} =
\begin{pmatrix}
\frac{\partial L}{\partial w_{11}} & \frac{\partial L}{\partial w_{12}} & \frac{\partial L}{\partial w_{13}} \\
\frac{\partial L}{\partial w_{21}} & \frac{\partial L}{\partial w_{22}} & \frac{\partial L}{\partial w_{23}}
\end{pmatrix}\tag{5}
$$</section><footer class=article-footer></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=https://lunatide.tech/p/cs229-lecture-7/><div class=article-image><img src=https://lunatide.tech/p/cs229-lecture-7/pic1.e3fe3f7deb19da40529efe3fac8bb5cd_hu_aa0783468c169fc4.jpg width=250 height=150 loading=lazy alt="Featured image of post CS229 Lecture 7" data-hash="md5-4/4/fesZ2kBSnv4/rIu1zQ=="></div><div class=article-details><h2 class=article-title>CS229 Lecture 7</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs229-lecture-6/><div class=article-image><img src=https://lunatide.tech/p/cs229-lecture-6/pic1.e3fe3f7deb19da40529efe3fac8bb5cd_hu_aa0783468c169fc4.jpg width=250 height=150 loading=lazy alt="Featured image of post CS229 Lecture 6" data-hash="md5-4/4/fesZ2kBSnv4/rIu1zQ=="></div><div class=article-details><h2 class=article-title>CS229 Lecture 6</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs229-lecture-52/><div class=article-image><img src=https://lunatide.tech/p/cs229-lecture-52/pic1.e3fe3f7deb19da40529efe3fac8bb5cd_hu_aa0783468c169fc4.jpg width=250 height=150 loading=lazy alt="Featured image of post CS229 Lecture 5(2)" data-hash="md5-4/4/fesZ2kBSnv4/rIu1zQ=="></div><div class=article-details><h2 class=article-title>CS229 Lecture 5(2)</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs229-lecture-5/><div class=article-image><img src=https://lunatide.tech/p/cs229-lecture-5/pic1.e3fe3f7deb19da40529efe3fac8bb5cd_hu_aa0783468c169fc4.jpg width=250 height=150 loading=lazy alt="Featured image of post CS229 Lecture 5" data-hash="md5-4/4/fesZ2kBSnv4/rIu1zQ=="></div><div class=article-details><h2 class=article-title>CS229 Lecture 5</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs229-lecture-4/><div class=article-image><img src=https://lunatide.tech/p/cs229-lecture-4/pic1.e3fe3f7deb19da40529efe3fac8bb5cd_hu_aa0783468c169fc4.jpg width=250 height=150 loading=lazy alt="Featured image of post CS229 Lecture 4" data-hash="md5-4/4/fesZ2kBSnv4/rIu1zQ=="></div><div class=article-details><h2 class=article-title>CS229 Lecture 4</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2025 -
2026 LunaTide's Blog</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=https://lunatide.tech/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>