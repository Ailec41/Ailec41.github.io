<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="deep learning note"><title>鱼书笔记-神经网络(下)</title><link rel=canonical href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/><link rel=stylesheet href=https://lunatide.tech/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="鱼书笔记-神经网络(下)"><meta property='og:description' content="deep learning note"><meta property='og:url' content='https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/'><meta property='og:site_name' content="LunaTide's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2025-11-12T00:00:00+00:00'><meta property='article:modified_time' content='2025-11-12T00:00:00+00:00'><meta property='og:image' content='https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/pic3.jpg'><meta name=twitter:title content="鱼书笔记-神经网络(下)"><meta name=twitter:description content="deep learning note"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/pic3.jpg'><link rel="shortcut icon" href=https://lunatide.tech/./favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column compact"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=https://lunatide.tech/><img src=https://lunatide.tech/img/avatar_hu_8b96c345a57a41a4.jpeg width=300 height=297 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😓</span></figure><div class=site-meta><h1 class=site-name><a href=https://lunatide.tech/>LunaTide's Blog</a></h1><h2 class=site-description>Welcome to my Blog</h2></div></header><ol class=menu-social><li><a href=http://codeforces.com target=_blank title=codeforces rel=me><svg viewBox="0 0 24 24" id="code-forces"><path fill="#F44336" d="M24 19.5V12a1.5 1.5.0 00-1.5-1.5h-3A1.5 1.5.0 0018 12v7.5a1.5 1.5.0 001.5 1.5h3a1.5 1.5.0 001.5-1.5z"/><path fill="#2196F3" d="M13.5 21a1.5 1.5.0 001.5-1.5v-15A1.5 1.5.0 0013.5 3h-3C9.673 3 9 3.672 9 4.5v15c0 .828.673 1.5 1.5 1.5h3z"/><path fill="#FFC107" d="M0 19.5c0 .828.673 1.5 1.5 1.5h3A1.5 1.5.0 006 19.5V9A1.5 1.5.0 004.5 7.5h-3C.673 7.5.0 8.172.0 9v10.5z"/></svg></a></li><li><a href=https://www.zhihu.com target=_blank title=Zhihu rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-zhihu"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 6h6v12h-2l-2 2-1-2h-1z"/><path d="M4 12h6.5"/><path d="M10.5 6h-5"/><path d="M6 4c-.5 2.5-1.5 3.5-2.5 4.5"/><path d="M8 6v7c0 4.5-2 5.5-4 7"/><path d="M11 18l-3-5"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=https://lunatide.tech/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=https://lunatide.tech/%E5%85%B3%E4%BA%8E/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li><a href=https://lunatide.tech/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=https://lunatide.tech/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=https://lunatide.tech/%E5%8F%8B%E9%93%BE/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>友链</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/pic3_hu_7244c689e98ede3c.jpg srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/pic3_hu_7244c689e98ede3c.jpg 800w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/pic3_hu_1ef3c590ecabf062.jpg 1600w" width=800 height=446 loading=lazy alt="Featured image of post 鱼书笔记-神经网络(下)"></a></div><div class=article-details><header class=article-category><a href=https://lunatide.tech/categories/deep-learning/>Deep Learning</a></header><div class=article-title-wrapper><h2 class=article-title><a href=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/>鱼书笔记-神经网络(下)</a></h2><h3 class=article-subtitle>deep learning note</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2025-11-12</time></div></footer></div></header><section class=article-content><p><strong>以下内容皆基于鱼书《深度学习入门基于python的理论与实现》</strong></p><h2 id=3层神经网络的实现>3层神经网络的实现</h2><p>开始进行神经网络的实现，以下图的三层神经网络为例</p><p><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p1.jpg width=1042 height=544 srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p1_hu_6ed3140333fbfe21.jpg 480w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p1_hu_ef4920c88ab22d8c.jpg 1024w" loading=lazy class=gallery-image data-flex-grow=191 data-flex-basis=459px></p><h3 id=符号确认>符号确认</h3><p>首先导入符号$w_{12}^{(1)}$, $a_{1}^{(1)}$等，如下图，权重和隐藏层的神经元右上角有一个"(1)"，它表示权重和神经元的层号，此外，权重右下角的两个数字，它们是后一层的神经元和前一层的神经元的索引号，比如$w_{12}^{(1)}$表示前一层的第二个神经元$x_2$到后一层的第1个神经元$a_{1}^{(1)}$的权重。权重右下角按照"后一层的索引号、前一层的索引号"的顺序排序</p><p><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p2.jpg width=1016 height=482 srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p2_hu_c622ef8dc25b755.jpg 480w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p2_hu_5a87340b1a46af20.jpg 1024w" loading=lazy class=gallery-image data-flex-grow=210 data-flex-basis=505px></p><h3 id=各层间信号传递的实现>各层间信号传递的实现</h3><p><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p3.jpg width=1020 height=624 srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p3_hu_eb6e8c3a1231341d.jpg 480w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p3_hu_6260c851cbcb0362.jpg 1024w" loading=lazy alt=从输入层到第1层的信号传递 class=gallery-image data-flex-grow=163 data-flex-basis=392px></p><p>上图增加了表示偏置的神经元"1"。偏置的右下角的索引号只有一个因为前一层的偏置神经元只有一个</p><p>现在通过加权信号和偏置的和计算表示$a_{1}^{(1)}$。</p>$$
a_{1}^{(1)} = w_{11}^{(1)} x_{1} + w_{12}^{(1)} x_{2} + b_{1}^{(1)}\tag{8}
$$<p>如果用矩阵的乘法运算，则可以将第1层的加权和表示成下面的式(9)</p>$$
A^{(1)} = XW^{(1)} + B^{(1)}
\tag{9}
$$<p>其中，$A^{(1)}$、$X$、$B^{(1)}$、$W^{(1)}$ 如下所示：</p>$$
A^{(1)} = \begin{pmatrix}
a_{1}^{(1)} & a_{2}^{(1)} & a_{3}^{(1)}
\end{pmatrix},
\quad
X = \begin{pmatrix}
x_1 & x_2
\end{pmatrix},
\quad
B^{(1)} = \begin{pmatrix}
b_{1}^{(1)} & b_{2}^{(1)} & b_{3}^{(1)}
\end{pmatrix}
$$$$
W^{(1)} = \begin{pmatrix}
w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\
w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}
\end{pmatrix}
$$<p>然后用NumPy多维数组来实现式(9)，输入信号，权重，偏置设置成任意值</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>1.0</span><span class=p>,</span><span class=mf>0.5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>W1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mf>0.1</span><span class=p>,</span><span class=mf>0.3</span><span class=p>,</span><span class=mf>0.5</span><span class=p>],[</span><span class=mf>0.2</span><span class=p>,</span><span class=mf>0.4</span><span class=p>,</span><span class=mf>0.6</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>B1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>0</span><span class=p>,</span><span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>W1</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1># (2,3)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1>#(2,)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>B1</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1>#(3.)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>A1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>W1</span><span class=p>)</span> <span class=o>+</span> <span class=n>B1</span>
</span></span></code></pre></td></tr></table></div></div><p>W1是2x3的数组，X是元素个数为2的一维数组。这里，W1和X的对应维度的元素个数也保持了一致。</p><p>然后我们用python来实现第一层激活函数的计算过程</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Z1</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>A1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>A1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>Z1</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>这里说的sigmoid函数就是之前定义的那个，它会接收NumPy数组，然后返回元素个数相同的NumPy数组</p><p>下面我们来实现第1层到第2层的信号传递</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>W2</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mf>0.1</span><span class=p>,</span><span class=mf>0.4</span><span class=p>],[</span><span class=mf>0.2</span><span class=p>,</span><span class=mf>0.5</span><span class=p>],[</span><span class=mf>0.3</span><span class=p>,</span><span class=mf>0.6</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>B2</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span><span class=mf>0.2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>Z1</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1>#(3,)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>W2</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1>#(3,2)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>B2</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1>#(2,)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>A2</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>Z1</span><span class=p>,</span><span class=n>W2</span><span class=p>)</span> <span class=o>+</span> <span class=n>B2</span>
</span></span><span class=line><span class=cl><span class=n>Z2</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>A2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>除了第一层的输出变成了第二层的输入，这个实现和刚才的一样</p><p>最后是第二层到输出层的信号传递，输出层的实现也和之前的实现基本相同，不过，最后的激活函数和之前的隐藏层有所不同</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>identity_function</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>W3</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span><span class=mf>0.3</span><span class=p>],[</span><span class=mf>0.2</span><span class=p>,</span><span class=mf>0.4</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>B3</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span><span class=mf>0.2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>A3</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>Z2</span><span class=p>,</span><span class=n>W3</span><span class=p>)</span> <span class=o>+</span> <span class=n>B3</span>
</span></span><span class=line><span class=cl><span class=n>Y</span> <span class=o>=</span> <span class=n>identity_function</span><span class=p>(</span><span class=n>A3</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>这里定义了<code>identity_function()</code>函数（恒等函数），并将其作为输出层的激活函数。</p><h3 id=代码总结>代码总结</h3><p>按照神经网络的实现惯例，把权重记为大写字母W1，其他都用小写字母表示</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>init_network</span><span class=p>():</span>
</span></span><span class=line><span class=cl>  <span class=n>network</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>  <span class=n>network</span><span class=p>[</span><span class=s1>&#39;W1&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>],</span> <span class=p>[</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.4</span><span class=p>,</span> <span class=mf>0.6</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>	<span class=n>network</span><span class=p>[</span><span class=s1>&#39;b1&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>])</span>
</span></span><span class=line><span class=cl>	<span class=n>network</span><span class=p>[</span><span class=s1>&#39;W2&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.4</span><span class=p>],[</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>],[</span><span class=mf>0.3</span><span class=p>,</span> <span class=mf>0.6</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>	<span class=n>network</span><span class=p>[</span><span class=s1>&#39;b2&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>	<span class=n>network</span><span class=p>[</span><span class=s1>&#39;W3&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>],[</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.4</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>  <span class=n>network</span><span class=p>[</span><span class=s1>&#39;b3&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>network</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>network</span><span class=p>,</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>W1</span><span class=p>,</span> <span class=n>W2</span><span class=p>,</span> <span class=n>W3</span> <span class=o>=</span> <span class=n>network</span><span class=p>[</span><span class=s1>&#39;W1&#39;</span><span class=p>],</span><span class=n>network</span><span class=p>[</span><span class=s1>&#39;W2&#39;</span><span class=p>],</span><span class=n>network</span><span class=p>[</span><span class=s1>&#39;W3&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=n>b1</span><span class=p>,</span> <span class=n>b2</span><span class=p>,</span> <span class=n>b3</span> <span class=o>=</span> <span class=n>network</span><span class=p>[</span><span class=s1>&#39;b1&#39;</span><span class=p>],</span><span class=n>network</span><span class=p>[</span><span class=s1>&#39;b2&#39;</span><span class=p>],</span><span class=n>network</span><span class=p>[</span><span class=s1>&#39;b3&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=n>a1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W1</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1</span>
</span></span><span class=line><span class=cl>	<span class=n>z1</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>a1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>a2</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>z1</span><span class=p>,</span> <span class=n>W2</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>	<span class=n>z2</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>a2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>a3</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>z2</span><span class=p>,</span> <span class=n>W3</span><span class=p>)</span> <span class=o>+</span> <span class=n>b3</span>
</span></span><span class=line><span class=cl>	<span class=n>y</span> <span class=o>=</span> <span class=n>identity_function</span><span class=p>(</span><span class=n>a3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>network</span> <span class=o>=</span> <span class=n>init_network</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>forward</span><span class=p>(</span><span class=n>network</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=c1># [ 0.31682708  0.69627909]</span>
</span></span></code></pre></td></tr></table></div></div><p>这里定义了<code>init_network()</code>和<code>forward()</code>函数，<code>init_network()</code>函数会进行权重和偏置的初始化，并将它们保存在字典变量network中。<code>forward()</code>函数中则封装了将输入信号转换为输出信号的处理过程</p><h2 id=输出层的设计>输出层的设计</h2><p>神经网络要根据情况改变输出层的激活函数。一般而言，回归问题用恒等函数，分类问题用softmax函数。</p><h3 id=恒等函数和softmax函数>恒等函数和softmax函数</h3><p>恒等函数会将输入按原样输出</p><p><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p4.jpg width=458 height=258 srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p4_hu_6d65e9351df62c95.jpg 480w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p4_hu_43b265d1cfa56ce1.jpg 1024w" loading=lazy alt=恒等函数 class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>分类问题中的softmax函数可以用下面的式(10)表示</p>$$
y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}\tag{10}
$$<p>上式表示假设输出层共有n个神经元，计算第k个神经元的输出$y_k$，如式(10)所示，softmax函数的分子是输入信号$a_k$的指数函数，分母是所有输入信号的指数函数的和</p><p><img src=https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p5.jpg width=410 height=262 srcset="https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p5_hu_9b7e96258f08abca.jpg 480w, https://lunatide.tech/p/%E9%B1%BC%E4%B9%A6%E7%AC%94%E8%AE%B0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8B/p5_hu_6d634ae849e8b735.jpg 1024w" loading=lazy class=gallery-image data-flex-grow=156 data-flex-basis=375px></p><p>接下来来实现softmax函数。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax</span><span class=p>(</span><span class=n>a</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>exp_a</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>sum_exp_a</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>exp_a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>y</span> <span class=o>=</span> <span class=n>exp_a</span> <span class=o>/</span> <span class=n>sub_exp_a</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>y</span><span class=p>;</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=实现softmax函数时的注意事项>实现softmax函数时的注意事项</h3><p>上面的softmax函数在计算上有一定的缺陷，就是溢出的问题，softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大，比如$e^{1000}$的结果会返回一个表示无穷大大inf，在这些超大值之间进行除法运算，结果会出现不确定的情况，softmax可以像如下(11)改进</p>$$
\begin{aligned}
y_k &= \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}
= \frac{C \exp(a_k)}{C \sum_{i=1}^{n} \exp(a_i)} \\[6pt]
&= \frac{\exp(a_k + \log C)}{\sum_{i=1}^{n} \exp(a_i + \log C)} \\[6pt]
&= \frac{\exp(a_k + C')}{\sum_{i=1}^{n} \exp(a_i + C')}
\end{aligned}
\tag{11}
$$<p>先在分子和分母上都乘以C（一个任意的常数），然后把C移动到指数函数中，记为$log C$。最后把$logC$替换为另外一个符号$C'$</p><p>综上，我们来实现下最终版的softmax函数</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax</span><span class=p>(</span><span class=n>a</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>c</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>exp_a</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>a</span> <span class=o>-</span> <span class=n>c</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>sum_exp_a</span> <span class=o>=</span> <span class=n>np_sum</span><span class=p>(</span><span class=n>exp_a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>y</span> <span class=o>=</span> <span class=n>exp_a</span> <span class=o>/</span> <span class=n>sum_exp_a</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=softmax函数的特征>softmax函数的特征</h3><ul><li>输出总和为1，因为这个性质我们才可以把softmax函数的输出解释为“概率”</li><li>使用了softmax函数各个元素之间的大小关系也不会改变，因为exp是单调递增的</li><li>神经网络一般只会把输出值最大的神经元所对应的类别作为识别结果。使用softmax函数输出值最大的神经元的位置也不会变，因此输出层的softmax函数一般会被忽略</li></ul><h3 id=输出层的神经元数量>输出层的神经元数量</h3><p>输出层的神经元数量需要根据待解决的问题来决定。对于分类问题，输出层的神经元数量一般设定为类别的数量。比如，对于某个输入图像，预测是图中的数字0到9中的哪个的问题，可以把输出层的神经元设定为10个，然后把这十个神经元按照从上到下，从0-9依次编号，并且值用不同的灰度表示，颜色越深，输出的值就越大</p><h2 id=手写数字识别>手写数字识别</h2><p>开始解决实际问题，假设学习已经全部结束，我们使用学习到的参数，先实现神经网络的“推理处理”。这个推理处理也称为神经网络的<strong>前向传播</strong></p><h3 id=mnist数据集>MNIST数据集</h3><p>MNIST数据集是由0到9的数字图像构成的。训练图像有6万张，测试图像有1万张，这些图像可以用于学习与推理。MNIST数据集的一般使用方法是，先用训练图像进行学习，再用学习到的模型度量能在能在多大程度上对测试图像进行正确的分类</p><p>MNIST的图像数据是28像素x28像素的灰度图像（1通道），各个像素的取值在0到255之间。每个图像都相应地标有“7” “2” “1”等标签。</p><h2 id=从数据中学习>从数据中学习</h2><h3 id=数据驱动>数据驱动</h3><p>如何实现数字“5”的识别，如果要设计一个能将5正确分类的程序</p></section><footer class=article-footer></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=https://lunatide.tech/p/cs229-lecture-3/><div class=article-image><img src=https://lunatide.tech/p/cs229-lecture-3/pic1.e3fe3f7deb19da40529efe3fac8bb5cd_hu_31804b513141a106.jpg width=250 height=150 loading=lazy alt="Featured image of post CS229 Lecture 3" data-hash="md5-4/4/fesZ2kBSnv4/rIu1zQ=="></div><div class=article-details><h2 class=article-title>CS229 Lecture 3</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs229%E4%BD%9C%E4%B8%9A0/><div class=article-image><img src=https://lunatide.tech/p/cs229%E4%BD%9C%E4%B8%9A0/pic1.e3fe3f7deb19da40529efe3fac8bb5cd_hu_31804b513141a106.jpg width=250 height=150 loading=lazy alt="Featured image of post CS229作业0" data-hash="md5-4/4/fesZ2kBSnv4/rIu1zQ=="></div><div class=article-details><h2 class=article-title>CS229作业0</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs229-lecture-2/><div class=article-image><img src=https://lunatide.tech/p/cs229-lecture-2/pic1.e3fe3f7deb19da40529efe3fac8bb5cd_hu_31804b513141a106.jpg width=250 height=150 loading=lazy alt="Featured image of post CS229 Lecture 2" data-hash="md5-4/4/fesZ2kBSnv4/rIu1zQ=="></div><div class=article-details><h2 class=article-title>CS229 Lecture 2</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs231-%E7%AC%AC%E5%9B%9B%E8%AE%B2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/><div class=article-image><img src=https://lunatide.tech/p/cs231-%E7%AC%AC%E5%9B%9B%E8%AE%B2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/pic1.543c3ec3703e3e550061d7dc2576aef5_hu_247d997126bf848d.jpg width=250 height=150 loading=lazy alt="Featured image of post CS231 第四讲 神经网络与反向传播" data-hash="md5-VDw+w3A+PlUAYdfcJXau9Q=="></div><div class=article-details><h2 class=article-title>CS231 第四讲 神经网络与反向传播</h2></div></a></article><article class=has-image><a href=https://lunatide.tech/p/cs231-%E7%AC%AC%E4%B8%89%E8%AE%B2-%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E4%BC%98%E5%8C%96/><div class=article-image><img src=https://lunatide.tech/p/cs231-%E7%AC%AC%E4%B8%89%E8%AE%B2-%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E4%BC%98%E5%8C%96/pic1.543c3ec3703e3e550061d7dc2576aef5_hu_247d997126bf848d.jpg width=250 height=150 loading=lazy alt="Featured image of post CS231 第三讲 正则化与优化" data-hash="md5-VDw+w3A+PlUAYdfcJXau9Q=="></div><div class=article-details><h2 class=article-title>CS231 第三讲 正则化与优化</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 LunaTide's Blog</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=https://lunatide.tech/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>